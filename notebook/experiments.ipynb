{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed2155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "import os \n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "loader = TextLoader(\"../data/info.txt\")\n",
    "documents = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a9c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bed82af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saipr\\AppData\\Local\\Temp\\ipykernel_21360\\100986205.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014c035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.invoke(query)\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29b8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",streaming=True\n",
    ")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a smart and knowledgeable AI assistant helping users understand the professional background, projects, skills, and certifications of Katta Sai Pranav Reddy.\n",
    "\n",
    "Use the following context extracted from Pranav's profile and provide a clear, helpful, and detailed answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": custom_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a3106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (streaming):\n",
      "To get in touch with Katta Sai Pranav Reddy, you can use the following contact information:\n",
      "\n",
      "* Phone: +91 93475 41040\n",
      "* Email: [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\n",
      "* GitHub: [github.com/ka1817](https://github.com/ka1817)\n",
      "* LinkedIn: [linkedin.com/in/pranav-reddy-katta](https://www.linkedin.com/in/pranav-reddy-katta/)\n",
      "\n",
      "Feel free to reach out to him through any of these channels."
     ]
    }
   ],
   "source": [
    "# --- 8. Run with streaming ---\n",
    "query = \"Give me contact Info\"\n",
    "\n",
    "print(\"Answer (streaming):\")\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7de6a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (streaming):\n",
      "The GitHub link of Pranav Reddy is not explicitly provided, but the GitHub repository for his project, \"BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\", can be accessed through the link: ðŸ” GitHub Repo: BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping. \n",
      "\n",
      "However, a more direct link is not provided in the context. Nevertheless, another GitHub link for his personal project, \"Netflix Customer Churn Prediction â€“ End-to-End ML System\", is mentioned as *[GitHub]*, but the actual link is not provided. \n",
      "\n",
      "If you are looking to access Pranav Reddy's GitHub profile or repositories, you may need to search for his username, which could be \"pranavreddy123\" based on his DockerHub repository (ðŸ³ DockerHub: pranavreddy123/bigbasket-assistant)."
     ]
    }
   ],
   "source": [
    "query = \"Give me the github link of pranav reddy\"\n",
    "\n",
    "print(\"Answer (streaming):\")\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7483e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (streaming):\n",
      "Katta Sai Pranav Reddy deployed the Netflix Customer Churn Prediction project using a containerized approach with Docker. The project utilizes a production-grade, explainable, and reproducible Machine Learning (ML) pipeline that incorporates various tools and technologies for efficient deployment.\n",
      "\n",
      "Here's an overview of the deployment process:\n",
      "\n",
      "1. **CI/CD**: The project implements Continuous Integration/Continuous Deployment (CI/CD) to ensure seamless and automated testing, building, and deployment of the ML model.\n",
      "2. **Experiment Tracking (MLflow)**: MLflow is used to track experiments, manage models, and monitor performance. This allows for easy comparison of different models, hyperparameters, and experiment results.\n",
      "3. **Data Versioning (DVC)**: DVC is used for data versioning, which enables the tracking of changes to the data and ensures reproducibility of the results.\n",
      "4. **Docker**: The project is containerized using Docker, which provides a lightweight and portable way to deploy the ML pipeline. This ensures that the deployment is consistent across different environments.\n",
      "5. **FastAPI Interface**: The deployed model is exposed through a FastAPI interface, which provides a simple and efficient way to interact with the model and retrieve predictions.\n",
      "\n",
      "By using this combination of tools and technologies, Katta Sai Pranav Reddy has created a robust and scalable deployment process for the Netflix Customer Churn Prediction project, enabling easy maintenance, updates, and reproduction of the results."
     ]
    }
   ],
   "source": [
    "query = \"How He Deployed Netflix Churn Prediction Project\"\n",
    "\n",
    "print(\"Answer (streaming):\")\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371ec80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saipr\\AppData\\Local\\Temp\\ipykernel_21360\\4188231742.py:92: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  initial_docs = self.base_retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 137\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    136\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are Pranav\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms main technical skills?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 137\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3045\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   3046\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3047\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3048\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3049\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3777\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3772\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3773\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3774\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3775\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3776\u001b[0m         ]\n\u001b[1;32m-> 3777\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3778\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3777\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3772\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3773\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3774\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3775\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3776\u001b[0m         ]\n\u001b[1;32m-> 3777\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3778\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3761\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[1;34m(step, input_, config, key)\u001b[0m\n\u001b[0;32m   3755\u001b[0m child_config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   3756\u001b[0m     config,\n\u001b[0;32m   3757\u001b[0m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[0;32m   3758\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   3759\u001b[0m )\n\u001b[0;32m   3760\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m-> 3761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3763\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3765\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\retrievers.py:265\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m    263\u001b[0m         )\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    267\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n",
      "Cell \u001b[1;32mIn[9], line 92\u001b[0m, in \u001b[0;36mRerankRetriever._get_relevant_documents\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_relevant_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m---> 92\u001b[0m     initial_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     reranked_docs \u001b[38;5;241m=\u001b[39m rerank_documents(query, initial_docs)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reranked_docs[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k]\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:190\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     emit_warning()\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\retrievers.py:414\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_name:\n\u001b[0;32m    413\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(query, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 261\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m    263\u001b[0m     )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1078\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs \u001b[38;5;241m|\u001b[39m kwargs\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1080\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1082\u001b[0m     docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1084\u001b[0m             query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m   1085\u001b[0m         )\n\u001b[0;32m   1086\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    625\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    630\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 643\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\n\u001b[0;32m    644\u001b[0m         query, k, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m, fetch_k\u001b[38;5;241m=\u001b[39mfetch_k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    645\u001b[0m     )\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:516\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    515\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(query)\n\u001b[1;32m--> 516\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[0;32m    517\u001b[0m     embedding,\n\u001b[0;32m    518\u001b[0m     k,\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[0;32m    520\u001b[0m     fetch_k\u001b[38;5;241m=\u001b[39mfetch_k,\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    522\u001b[0m )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:428\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    427\u001b[0m _id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_docstore_id[i]\n\u001b[1;32m--> 428\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m(_id)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, Document):\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find document for id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 0. API Key ---\n",
    "if not os.getenv(\"GROQ_API_KEY\"):\n",
    "    raise ValueError(\"Please set GROQ_API_KEY in your environment\")\n",
    "\n",
    "\n",
    "# --- 1. Load & Split Documents ---\n",
    "loader = TextLoader(\"../data/info.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "texts = [d.page_content for d in docs]\n",
    "\n",
    "\n",
    "# --- 2. Embeddings ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "vectors = embeddings.embed_documents(texts)  # list of embeddings\n",
    "dim = len(vectors[0])\n",
    "\n",
    "\n",
    "# --- 3. Build FAISS IVF Index ---\n",
    "nlist = 5   # number of clusters\n",
    "quantizer = faiss.IndexFlatL2(dim)\n",
    "index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_L2)\n",
    "\n",
    "# IVF requires training\n",
    "index.train(np.array(vectors).astype(\"float32\"))\n",
    "\n",
    "# Add vectors\n",
    "index.add(np.array(vectors).astype(\"float32\"))\n",
    "\n",
    "# Default search: probe only a few clusters\n",
    "index.nprobe = 10\n",
    "\n",
    "\n",
    "# --- 4. Wrap IVF Index into LangChain ---\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "# Map documents to their vector IDs\n",
    "docstore = {}\n",
    "index_to_docstore_id = {}\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_id = str(i)\n",
    "    docstore[doc_id] = doc\n",
    "    index_to_docstore_id[i] = doc_id\n",
    "\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. CrossEncoder Reranker ---\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs).tolist()\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "\n",
    "# --- 6. Custom Reranker Retriever ---\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # allow FAISS retriever\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.get_relevant_documents(query)\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "\n",
    "\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=3)\n",
    "\n",
    "\n",
    "# --- 7. LLM ---\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", streaming=True)\n",
    "\n",
    "\n",
    "# --- 8. Prompt ---\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a smart and knowledgeable AI assistant helping users understand the professional background, projects, skills, and certifications of Katta Sai Pranav Reddy.\n",
    "\n",
    "Use the following context extracted from Pranav's profile and provide a clear, helpful, and detailed answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "# --- 9. RAG Chain ---\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": custom_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# --- ðŸ”¹ Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"What are Pranav's main technical skills?\"\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d6fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a326883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Pranav's main technical skills can be categorized into several areas:\n",
      "\n",
      "1. **Tools:** He is proficient in using tools such as:\n",
      "\t* MLflow\n",
      "\t* DVC\n",
      "\t* Docker\n",
      "\t* Git\n",
      "\t* GitHub Actions\n",
      "\t* AWS (EC2, S3, ECR)\n",
      "\t* FAISS\n",
      "\t* Pinecone\n",
      "\t* Hugging Face\n",
      "\t* LangChain\n",
      "\t* LangSmith\n",
      "\t* FastAPI\n",
      "2. **Programming & Technical Skills:** He has expertise in:\n",
      "\t* Python\n",
      "\t* SQL\n",
      "\t* HTML\n",
      "\t* CSS\n",
      "\t* Scikit-learn\n",
      "\t* TensorFlow\n",
      "\t* Keras\n",
      "\t* Statistics\n",
      "3. **Data Science & Machine Learning:** He is skilled in:\n",
      "\t* Data Preprocessing\n",
      "\t* Exploratory Data Analysis (EDA)\n",
      "\t* Feature Engineering\n",
      "\t* Model Training & Evaluation\n",
      "\t* Hyperparameter Tuning\n",
      "\t* Clustering\n",
      "\t* MLOps\n",
      "\t* Semantic Search\n",
      "\t* Retrieval-Augmented Generation (RAG)\n",
      "\t* CNN\n",
      "\t* RNN\n",
      "\t* GPT\n",
      "\t* Transformers\n",
      "\t* Fine-Tuning\n",
      "\t* Prompt Engineering\n",
      "4. **Data Visualization & Analysis:** He is proficient in:\n",
      "\t* Pandas\n",
      "\t* NumPy\n",
      "\t* Matplotlib\n",
      "\t* Seaborn\n",
      "\n",
      "These technical skills demonstrate Pranav's strong foundation in data science, machine learning, and programming, making him a versatile and skilled professional in the field.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os \n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "loader = TextLoader(\"../data/info.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "vectors = embeddings.embed_documents([d.page_content for d in docs])\n",
    "dim = len(vectors[0])\n",
    "num_docs = len(vectors)\n",
    "if num_docs > 20:  \n",
    "    nlist = min(10, num_docs // 2)  \n",
    "    quantizer = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_L2)\n",
    "\n",
    "    index.train(np.array(vectors).astype(\"float32\"))\n",
    "    index.add(np.array(vectors).astype(\"float32\"))\n",
    "\n",
    "    index.nprobe = max(1, nlist // 2)\n",
    "else:\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(vectors).astype(\"float32\"))\n",
    "\n",
    "docstore = InMemoryDocstore()\n",
    "index_to_docstore_id = {}\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_id = str(i)\n",
    "    docstore.add({doc_id: doc})\n",
    "    index_to_docstore_id[i] = doc_id\n",
    "\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    ")\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.invoke(query)\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=3)\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a smart and knowledgeable AI assistant helping users understand \n",
    "the professional background, projects, skills, and certifications of Katta Sai Pranav Reddy.\n",
    "\n",
    "Use the following context extracted from Pranav's profile and provide a clear, helpful, and detailed answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": custom_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"What are Pranav's main technical skills?\"\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06bde69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Pranav Reddy deployed his BigBasket SmartCart application using a combination of containerization, automation, and cloud computing. Here's a step-by-step overview of his deployment process:\n",
      "\n",
      "1. **Containerization using Docker**: Pranav containerized his application using Docker, which allowed him to package the application and its dependencies into a single container. This ensured that the application was isolated and portable, making it easy to deploy on different environments.\n",
      "2. **Automated CI/CD using GitHub Actions**: Pranav used GitHub Actions to automate his Continuous Integration and Continuous Deployment (CI/CD) pipeline. He created a workflow file (.github/workflows/ci-cd.yml) that defined the build, test, and deployment process. The workflow was triggered on push events to the main branch or pull requests.\n",
      "3. **Building and Pushing Docker Image**: As part of the CI/CD pipeline, GitHub Actions built the Docker image and pushed it to DockerHub (pranavreddy123/bigbasket-assistant:latest).\n",
      "4. **Deployment on Amazon EC2**: Pranav launched an Ubuntu 20.04 EC2 instance on Amazon Web Services (AWS) and installed Docker on the instance.\n",
      "5. **Running the Docker Container**: Pranav pulled the Docker image from DockerHub and ran the container on the EC2 instance, mapping port 8000 on the host machine to port 8000 in the container.\n",
      "6. **Environment Variable Configuration**: Pranav set an environment variable (GROQ_API_KEY) on the EC2 instance to configure the application.\n",
      "7. **Accessing the Application**: Finally, Pranav accessed his BigBasket SmartCart application by visiting the EC2 instance's public IP address on port 8000 (http://<your-ec2-public-ip>:8000).\n",
      "\n",
      "By following this deployment process, Pranav was able to ensure a smooth, automated, and scalable deployment of his BigBasket SmartCart application.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"How Did Pranav Reddy Deployed His BigBasket SmartCart Application\"\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd07e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
