{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed2155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "import os \n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "loader = TextLoader(\"../data/info.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.invoke(query)\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=3)\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",streaming=True\n",
    ")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a smart and knowledgeable AI assistant helping users understand the professional background, projects, skills, and certifications of Katta Sai Pranav Reddy.\n",
    "\n",
    "Use the following context extracted from Pranav's profile and provide a clear, helpful, and detailed answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "# --- 7. RAG Chain ---\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": custom_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a3106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (streaming):\n",
      "Based on the provided context, here are the GitHub repository links for the projects developed by Katta Sai Pranav Reddy:\n",
      "\n",
      "1. **BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping**: https://github.com/PranavReddy/BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping (Note: The exact GitHub username is not provided in the context, so I assumed it to be \"PranavReddy\". The actual link may vary depending on the correct GitHub username.)\n",
      "2. **Netflix Customer Churn Prediction ‚Äì End-to-End ML System**: The exact GitHub repository link is not provided in the context. However, it is mentioned that the project is available on GitHub, and the link can be found on Pranav Reddy's GitHub profile.\n",
      "\n",
      "To find the exact links, you can search for Pranav Reddy's GitHub profile and look for the respective repository names."
     ]
    }
   ],
   "source": [
    "# --- 8. Run with streaming ---\n",
    "query = \"Give me Github repo links of projects\"\n",
    "\n",
    "print(\"Answer (streaming):\")\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7de6a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (streaming):\n",
      "The GitHub link of Pranav Reddy is not explicitly provided, but the GitHub repository for his project, \"BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\", can be accessed through the link: üîç GitHub Repo: BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping. \n",
      "\n",
      "However, a more direct link is not provided in the context. Nevertheless, another GitHub link for his personal project, \"Netflix Customer Churn Prediction ‚Äì End-to-End ML System\", is mentioned as *[GitHub]*, but the actual link is not provided. \n",
      "\n",
      "If you are looking to access Pranav Reddy's GitHub profile or repositories, you may need to search for his username, which could be \"pranavreddy123\" based on his DockerHub repository (üê≥ DockerHub: pranavreddy123/bigbasket-assistant)."
     ]
    }
   ],
   "source": [
    "query = \"Give me the github link of pranav reddy\"\n",
    "\n",
    "print(\"Answer (streaming):\")\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7483e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (streaming):\n",
      "Katta Sai Pranav Reddy deployed the Netflix Customer Churn Prediction project using a containerized approach with Docker. The project utilizes a production-grade, explainable, and reproducible Machine Learning (ML) pipeline that incorporates various tools and technologies for efficient deployment.\n",
      "\n",
      "Here's an overview of the deployment process:\n",
      "\n",
      "1. **CI/CD**: The project implements Continuous Integration/Continuous Deployment (CI/CD) to ensure seamless and automated testing, building, and deployment of the ML model.\n",
      "2. **Experiment Tracking (MLflow)**: MLflow is used to track experiments, manage models, and monitor performance. This allows for easy comparison of different models, hyperparameters, and experiment results.\n",
      "3. **Data Versioning (DVC)**: DVC is used for data versioning, which enables the tracking of changes to the data and ensures reproducibility of the results.\n",
      "4. **Docker**: The project is containerized using Docker, which provides a lightweight and portable way to deploy the ML pipeline. This ensures that the deployment is consistent across different environments.\n",
      "5. **FastAPI Interface**: The deployed model is exposed through a FastAPI interface, which provides a simple and efficient way to interact with the model and retrieve predictions.\n",
      "\n",
      "By using this combination of tools and technologies, Katta Sai Pranav Reddy has created a robust and scalable deployment process for the Netflix Customer Churn Prediction project, enabling easy maintenance, updates, and reproduction of the results."
     ]
    }
   ],
   "source": [
    "query = \"How He Deployed Netflix Churn Prediction Project\"\n",
    "\n",
    "print(\"Answer (streaming):\")\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
