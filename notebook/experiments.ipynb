{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed2155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "import os \n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "loader = TextLoader(\"../data/info.txt\")\n",
    "documents = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a9c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bed82af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saipr\\AppData\\Local\\Temp\\ipykernel_21360\\100986205.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014c035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.invoke(query)\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29b8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",streaming=True\n",
    ")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a smart and knowledgeable AI assistant helping users understand the professional background, projects, skills, and certifications of Katta Sai Pranav Reddy.\n",
    "\n",
    "Use the following context extracted from Pranav's profile and provide a clear, helpful, and detailed answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": custom_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a3106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (streaming):\n",
      "To get in touch with Katta Sai Pranav Reddy, you can use the following contact information:\n",
      "\n",
      "* Phone: +91 93475 41040\n",
      "* Email: [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\n",
      "* GitHub: [github.com/ka1817](https://github.com/ka1817)\n",
      "* LinkedIn: [linkedin.com/in/pranav-reddy-katta](https://www.linkedin.com/in/pranav-reddy-katta/)\n",
      "\n",
      "Feel free to reach out to him through any of these channels."
     ]
    }
   ],
   "source": [
    "# --- 8. Run with streaming ---\n",
    "query = \"Give me contact Info\"\n",
    "\n",
    "print(\"Answer (streaming):\")\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7de6a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (streaming):\n",
      "The GitHub link of Pranav Reddy is not explicitly provided, but the GitHub repository for his project, \"BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\", can be accessed through the link: üîç GitHub Repo: BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping. \n",
      "\n",
      "However, a more direct link is not provided in the context. Nevertheless, another GitHub link for his personal project, \"Netflix Customer Churn Prediction ‚Äì End-to-End ML System\", is mentioned as *[GitHub]*, but the actual link is not provided. \n",
      "\n",
      "If you are looking to access Pranav Reddy's GitHub profile or repositories, you may need to search for his username, which could be \"pranavreddy123\" based on his DockerHub repository (üê≥ DockerHub: pranavreddy123/bigbasket-assistant)."
     ]
    }
   ],
   "source": [
    "query = \"Give me the github link of pranav reddy\"\n",
    "\n",
    "print(\"Answer (streaming):\")\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7483e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (streaming):\n",
      "Katta Sai Pranav Reddy deployed the Netflix Customer Churn Prediction project using a containerized approach with Docker. The project utilizes a production-grade, explainable, and reproducible Machine Learning (ML) pipeline that incorporates various tools and technologies for efficient deployment.\n",
      "\n",
      "Here's an overview of the deployment process:\n",
      "\n",
      "1. **CI/CD**: The project implements Continuous Integration/Continuous Deployment (CI/CD) to ensure seamless and automated testing, building, and deployment of the ML model.\n",
      "2. **Experiment Tracking (MLflow)**: MLflow is used to track experiments, manage models, and monitor performance. This allows for easy comparison of different models, hyperparameters, and experiment results.\n",
      "3. **Data Versioning (DVC)**: DVC is used for data versioning, which enables the tracking of changes to the data and ensures reproducibility of the results.\n",
      "4. **Docker**: The project is containerized using Docker, which provides a lightweight and portable way to deploy the ML pipeline. This ensures that the deployment is consistent across different environments.\n",
      "5. **FastAPI Interface**: The deployed model is exposed through a FastAPI interface, which provides a simple and efficient way to interact with the model and retrieve predictions.\n",
      "\n",
      "By using this combination of tools and technologies, Katta Sai Pranav Reddy has created a robust and scalable deployment process for the Netflix Customer Churn Prediction project, enabling easy maintenance, updates, and reproduction of the results."
     ]
    }
   ],
   "source": [
    "query = \"How He Deployed Netflix Churn Prediction Project\"\n",
    "\n",
    "print(\"Answer (streaming):\")\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ec80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saipr\\AppData\\Local\\Temp\\ipykernel_21360\\4188231742.py:92: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  initial_docs = self.base_retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 137\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    136\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are Pranav\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms main technical skills?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 137\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3045\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   3046\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3047\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3048\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3049\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3777\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3772\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3773\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3774\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3775\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3776\u001b[0m         ]\n\u001b[1;32m-> 3777\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3778\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3777\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3772\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3773\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3774\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3775\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3776\u001b[0m         ]\n\u001b[1;32m-> 3777\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3778\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3761\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[1;34m(step, input_, config, key)\u001b[0m\n\u001b[0;32m   3755\u001b[0m child_config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   3756\u001b[0m     config,\n\u001b[0;32m   3757\u001b[0m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[0;32m   3758\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   3759\u001b[0m )\n\u001b[0;32m   3760\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m-> 3761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3763\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3765\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\retrievers.py:265\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m    263\u001b[0m         )\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    267\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n",
      "Cell \u001b[1;32mIn[9], line 92\u001b[0m, in \u001b[0;36mRerankRetriever._get_relevant_documents\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_relevant_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m---> 92\u001b[0m     initial_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     reranked_docs \u001b[38;5;241m=\u001b[39m rerank_documents(query, initial_docs)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reranked_docs[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k]\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:190\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     emit_warning()\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\retrievers.py:414\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_name:\n\u001b[0;32m    413\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(query, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 261\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m    263\u001b[0m     )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1078\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs \u001b[38;5;241m|\u001b[39m kwargs\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1080\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1082\u001b[0m     docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1084\u001b[0m             query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m   1085\u001b[0m         )\n\u001b[0;32m   1086\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    625\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    630\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 643\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\n\u001b[0;32m    644\u001b[0m         query, k, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m, fetch_k\u001b[38;5;241m=\u001b[39mfetch_k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    645\u001b[0m     )\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:516\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    515\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(query)\n\u001b[1;32m--> 516\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[0;32m    517\u001b[0m     embedding,\n\u001b[0;32m    518\u001b[0m     k,\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[0;32m    520\u001b[0m     fetch_k\u001b[38;5;241m=\u001b[39mfetch_k,\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    522\u001b[0m )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32mc:\\Users\\saipr\\Personal_Portfolio\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:428\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    427\u001b[0m _id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_docstore_id[i]\n\u001b[1;32m--> 428\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m(_id)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, Document):\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find document for id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"GROQ_API_KEY\"):\n",
    "    raise ValueError(\"Please set GROQ_API_KEY in your environment\")\n",
    "\n",
    "\n",
    "loader = TextLoader(\"../data/info.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "texts = [d.page_content for d in docs]\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "vectors = embeddings.embed_documents(texts)  \n",
    "dim = len(vectors[0])\n",
    "\n",
    "\n",
    "nlist = 5   \n",
    "quantizer = faiss.IndexFlatL2(dim)\n",
    "index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_L2)\n",
    "\n",
    "index.train(np.array(vectors).astype(\"float32\"))\n",
    "\n",
    "# Add vectors\n",
    "index.add(np.array(vectors).astype(\"float32\"))\n",
    "\n",
    "index.nprobe = 10\n",
    "\n",
    "\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "docstore = {}\n",
    "index_to_docstore_id = {}\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_id = str(i)\n",
    "    docstore[doc_id] = doc\n",
    "    index_to_docstore_id[i] = doc_id\n",
    "\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    ")\n",
    "\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs).tolist()\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  \n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.get_relevant_documents(query)\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "\n",
    "\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=3)\n",
    "\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", streaming=True)\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a smart and knowledgeable AI assistant helping users understand the professional background, projects, skills, and certifications of Katta Sai Pranav Reddy.\n",
    "\n",
    "Use the following context extracted from Pranav's profile and provide a clear, helpful, and detailed answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": custom_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"What are Pranav's main technical skills?\"\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d6fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a326883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Pranav's main technical skills can be categorized into several areas:\n",
      "\n",
      "1. **Tools:** He is proficient in using tools such as:\n",
      "\t* MLflow\n",
      "\t* DVC\n",
      "\t* Docker\n",
      "\t* Git\n",
      "\t* GitHub Actions\n",
      "\t* AWS (EC2, S3, ECR)\n",
      "\t* FAISS\n",
      "\t* Pinecone\n",
      "\t* Hugging Face\n",
      "\t* LangChain\n",
      "\t* LangSmith\n",
      "\t* FastAPI\n",
      "2. **Programming & Technical Skills:** He has expertise in:\n",
      "\t* Python\n",
      "\t* SQL\n",
      "\t* HTML\n",
      "\t* CSS\n",
      "\t* Scikit-learn\n",
      "\t* TensorFlow\n",
      "\t* Keras\n",
      "\t* Statistics\n",
      "3. **Data Science & Machine Learning:** He is skilled in:\n",
      "\t* Data Preprocessing\n",
      "\t* Exploratory Data Analysis (EDA)\n",
      "\t* Feature Engineering\n",
      "\t* Model Training & Evaluation\n",
      "\t* Hyperparameter Tuning\n",
      "\t* Clustering\n",
      "\t* MLOps\n",
      "\t* Semantic Search\n",
      "\t* Retrieval-Augmented Generation (RAG)\n",
      "\t* CNN\n",
      "\t* RNN\n",
      "\t* GPT\n",
      "\t* Transformers\n",
      "\t* Fine-Tuning\n",
      "\t* Prompt Engineering\n",
      "4. **Data Visualization & Analysis:** He is proficient in:\n",
      "\t* Pandas\n",
      "\t* NumPy\n",
      "\t* Matplotlib\n",
      "\t* Seaborn\n",
      "\n",
      "These technical skills demonstrate Pranav's strong foundation in data science, machine learning, and programming, making him a versatile and skilled professional in the field.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os \n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "loader = TextLoader(\"../data/info.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "vectors = embeddings.embed_documents([d.page_content for d in docs])\n",
    "dim = len(vectors[0])\n",
    "num_docs = len(vectors)\n",
    "if num_docs > 20:  \n",
    "    nlist = min(10, num_docs // 2)  \n",
    "    quantizer = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_L2)\n",
    "\n",
    "    index.train(np.array(vectors).astype(\"float32\"))\n",
    "    index.add(np.array(vectors).astype(\"float32\"))\n",
    "\n",
    "    index.nprobe = max(1, nlist // 2)\n",
    "else:\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(vectors).astype(\"float32\"))\n",
    "\n",
    "docstore = InMemoryDocstore()\n",
    "index_to_docstore_id = {}\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_id = str(i)\n",
    "    docstore.add({doc_id: doc})\n",
    "    index_to_docstore_id[i] = doc_id\n",
    "\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    ")\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.invoke(query)\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=3)\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a smart and knowledgeable AI assistant helping users understand \n",
    "the professional background, projects, skills, and certifications of Katta Sai Pranav Reddy.\n",
    "\n",
    "Use the following context extracted from Pranav's profile and provide a clear, helpful, and detailed answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": custom_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"What are Pranav's main technical skills?\"\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06bde69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Pranav Reddy deployed his BigBasket SmartCart application using a combination of containerization, automation, and cloud computing. Here's a step-by-step overview of his deployment process:\n",
      "\n",
      "1. **Containerization using Docker**: Pranav containerized his application using Docker, which allowed him to package the application and its dependencies into a single container. This ensured that the application was isolated and portable, making it easy to deploy on different environments.\n",
      "2. **Automated CI/CD using GitHub Actions**: Pranav used GitHub Actions to automate his Continuous Integration and Continuous Deployment (CI/CD) pipeline. He created a workflow file (.github/workflows/ci-cd.yml) that defined the build, test, and deployment process. The workflow was triggered on push events to the main branch or pull requests.\n",
      "3. **Building and Pushing Docker Image**: As part of the CI/CD pipeline, GitHub Actions built the Docker image and pushed it to DockerHub (pranavreddy123/bigbasket-assistant:latest).\n",
      "4. **Deployment on Amazon EC2**: Pranav launched an Ubuntu 20.04 EC2 instance on Amazon Web Services (AWS) and installed Docker on the instance.\n",
      "5. **Running the Docker Container**: Pranav pulled the Docker image from DockerHub and ran the container on the EC2 instance, mapping port 8000 on the host machine to port 8000 in the container.\n",
      "6. **Environment Variable Configuration**: Pranav set an environment variable (GROQ_API_KEY) on the EC2 instance to configure the application.\n",
      "7. **Accessing the Application**: Finally, Pranav accessed his BigBasket SmartCart application by visiting the EC2 instance's public IP address on port 8000 (http://<your-ec2-public-ip>:8000).\n",
      "\n",
      "By following this deployment process, Pranav was able to ensure a smooth, automated, and scalable deployment of his BigBasket SmartCart application.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"How Did Pranav Reddy Deployed His BigBasket SmartCart Application\"\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae28c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3.0\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "print(pinecone.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0aaecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_5fcYNp_SaAxqoMdnTB1CTT7Cm4hadkAYywfDLj94e3WDpWsHuU92XZSZJ6J2AakvyBHnrh\")\n",
    "index_name = \"developer-quickstart-py\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        embed={\n",
    "            \"model\":\"llama-text-embed-v2\",\n",
    "            \"field_map\":{\"text\": \"chunk_text\"}\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7426f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 62\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = \"latest-portfolio1\"  \n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "text_path = \"../data/info.txt\"\n",
    "loader = TextLoader(text_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Total Chunks: {len(docs)}\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"‚ùå Missing PINECONE_API_KEY in environment variables\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15677486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bcf92a67-d961-4369-9c24-8c9ba79b927a',\n",
       " '60ff68ba-b880-44cd-9152-9476a4b7b8aa',\n",
       " 'bb9853a9-f75d-48d1-a190-99851379d341',\n",
       " '117ef1ed-4552-44cf-b1d0-ba88f4fd021a',\n",
       " 'b6165dae-763b-4549-b3a4-8a4361cebb4d',\n",
       " '9806238b-2e87-4be8-ba69-268bd541968b',\n",
       " '29c54d14-d027-4eb4-bec6-d57e40334f1e',\n",
       " '89deb20e-25af-41b8-ab04-263c4e51d4f9',\n",
       " '10626073-7590-4d78-8cd3-3c17108e52a1',\n",
       " '0d87b496-e327-4b6c-994a-00ffdfc1e472',\n",
       " '754587c7-d989-4211-8d16-b8855d5fa92e',\n",
       " '26d04d19-b806-4a60-870b-37a70aad66c1',\n",
       " 'e89e3279-dc84-4357-8b73-110058b2c090',\n",
       " '88af8404-e669-4279-81bc-2083a39bd489',\n",
       " 'a0e2895a-f054-4e00-b920-1d3c14333f3e',\n",
       " '4bcf0e29-a25a-4a6a-a721-32ced55bc5da',\n",
       " '7f5dad96-f46e-48f6-8b48-218bf8549e64',\n",
       " '28f0a4d3-6335-4c60-85ef-704f4412b6fe',\n",
       " '31cef1b8-f5bc-42f3-b055-8174e97f5f62',\n",
       " '37837cae-f9e4-4310-b5a2-5cae6cfcea0a',\n",
       " '1b1290b5-875a-4557-87b5-3003bdc20192',\n",
       " '314ac472-571d-4b51-b4b0-d1f6a9141852',\n",
       " '3c99d35e-c709-44be-8dca-778bb0bbe30e',\n",
       " '14450ced-fece-4a97-b6e0-9a9bdb62447d',\n",
       " '4b2df0e2-fe42-42a4-8445-804b7a127ea8',\n",
       " 'fab6dfc0-b1e4-4945-99f2-f89981e4ef9f',\n",
       " '0818ece6-edf2-4eae-8638-c684ce64795b',\n",
       " '93882bbd-5e41-4afb-ba28-ccd675859aee',\n",
       " '10ee1665-100c-4ab9-8eb8-87c8f00698f7',\n",
       " 'bec056be-dac1-49b8-aa02-7dd902250eed',\n",
       " '4fa810d8-136b-4274-8a26-bba2535e1aa6',\n",
       " '75e10db9-c78a-4eea-bd99-50ed7eac62ad',\n",
       " 'cef66cd7-fdd1-442b-b33b-244dae2ee446',\n",
       " '2db8801c-9b57-4315-9e35-67d7b18efdb4',\n",
       " 'ab120b2f-e604-43a9-9c64-d143c9cbb7fb',\n",
       " 'c82b9126-e372-4c0c-88ec-62d3dfec5e87',\n",
       " '2c30ff07-b49c-472c-a8cd-e2fe3c3f966c',\n",
       " '11c0a3c9-543e-4fdb-a02c-9ad7a5312bcc',\n",
       " '8121a273-8a03-4d56-b9ff-ee96f4e87628',\n",
       " 'c36cc07b-e575-4595-a33a-61bf68dfefae',\n",
       " '4b130056-4f8c-4153-ae4c-6ad23416f1cd',\n",
       " '74bacc2a-9e0e-47e0-8494-5b94801dd848',\n",
       " 'a69cdd59-2cbb-42cf-ba2c-7440ef40fca0',\n",
       " '368e10f7-5a24-432d-a264-1e338136e496',\n",
       " '114af193-70ba-49b6-8911-f968556caf2d',\n",
       " '1fda22aa-9550-4898-a5e5-d4c9e27b9486',\n",
       " 'a9da3a2d-1a36-4784-a82c-7c8613aa192f',\n",
       " '36c5d5b5-2ccd-4122-8ff4-6f9be41270ad',\n",
       " '2e9f44fd-e3d1-4762-8343-7fc814bda31c',\n",
       " '55485318-af5e-4fea-ad12-db074d8219c9',\n",
       " 'e76e38fe-8f0a-4513-a259-8497c816bed5',\n",
       " '4ddb6346-2b01-45c8-ba96-e07860775e33',\n",
       " '7eddfca5-b4d6-4a08-9217-de6c5e5d06a9',\n",
       " 'fd577c42-be40-43cb-ac73-fc0e5420cf3e',\n",
       " 'cad77f7c-8064-4ef7-9036-e2ba208484db',\n",
       " 'a7df4bc0-4455-4798-92d8-024787a518a0',\n",
       " '3dde5c59-abbc-4e05-86cd-6e5a453f30d0',\n",
       " 'fcef9227-a845-4bca-93f3-b0630d376e2e',\n",
       " 'a9edfa95-87ef-4553-992e-11bf1a24cf27',\n",
       " 'a140fed3-463d-49d1-9654-18a83747e1f3',\n",
       " '8675afff-cb9e-469d-a847-ebe823484730',\n",
       " 'e24924b1-b85e-47f0-9b48-0ad82be75deb']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(docs))]\n",
    "\n",
    "vector_store.add_documents(documents=docs, ids=uuids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "921302fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x19a29505630>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4817b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/info.txt', 'text': \"10th class Marks\\n\\n**Board of Secondary Education\\nTelangana State, India**\\n\\n**SECONDARY SCHOOL CERTIFICATE**\\n**REGULAR** PC/29/4222/04/256517/3\\n**TS-EE 524495**\\n\\n---\\n\\n**CERTIFIED THAT**\\n**KATTA SAI PRANAV REDDY**\\n**Father's Name:** KATTA SRINIVAS REDDY\\n**Mother's Name:** KATTA UMARANI\\n**Roll No.:** 1929100642\\n**Date of Birth:** 03/06/2003 (Zero Three June Two Zero Zero Three)\\n**School:** EKALAVYA FOUNDATION SCL NALGONDA, NALGONDA DISTRICT\\n**Medium:** ENGLISH\\n\\nHas appeared and **PASSED SSC EXAMINATION** held in **MARCH‚Äì2019**\\n\\n\\n### **The Candidate Secured the Following Grade and Grade Points in Curricular Areas:**\"}, page_content=\"10th class Marks\\n\\n**Board of Secondary Education\\nTelangana State, India**\\n\\n**SECONDARY SCHOOL CERTIFICATE**\\n**REGULAR** PC/29/4222/04/256517/3\\n**TS-EE 524495**\\n\\n---\\n\\n**CERTIFIED THAT**\\n**KATTA SAI PRANAV REDDY**\\n**Father's Name:** KATTA SRINIVAS REDDY\\n**Mother's Name:** KATTA UMARANI\\n**Roll No.:** 1929100642\\n**Date of Birth:** 03/06/2003 (Zero Three June Two Zero Zero Three)\\n**School:** EKALAVYA FOUNDATION SCL NALGONDA, NALGONDA DISTRICT\\n**Medium:** ENGLISH\\n\\nHas appeared and **PASSED SSC EXAMINATION** held in **MARCH‚Äì2019**\\n\\n\\n### **The Candidate Secured the Following Grade and Grade Points in Curricular Areas:**\"),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'Has appeared and **PASSED SSC EXAMINATION** held in **MARCH‚Äì2019**\\n\\n\\n### **The Candidate Secured the Following Grade and Grade Points in Curricular Areas:**\\n\\n| Subject                  | Grade FA | Grade SA | Overall Grade | Grade Point |\\n| ------------------------ | -------- | -------- | ------------- | ----------- |\\n| First Language (TELUGU)  | A1       | A1       | A1            | 10          |\\n| Third Language (ENGLISH) | A1       | A2       | A1            | 10          |\\n| Mathematics              | A1       | A1       | A1            | 10          |\\n| Science                  | A1       | A2       | A1            | 09          |\\n| Social Studies           | A1       | A1       | A1            | 10          |\\n| Second Language (HINDI)  | A1       | B1       | B1            | 08          |\\n\\n\\n### **Cumulative Grade Point Average (CGPA): 9.5**\\n\\n\\n### **CO-CURRICULAR AREAS:**'}, page_content='Has appeared and **PASSED SSC EXAMINATION** held in **MARCH‚Äì2019**\\n\\n\\n### **The Candidate Secured the Following Grade and Grade Points in Curricular Areas:**\\n\\n| Subject                  | Grade FA | Grade SA | Overall Grade | Grade Point |\\n| ------------------------ | -------- | -------- | ------------- | ----------- |\\n| First Language (TELUGU)  | A1       | A1       | A1            | 10          |\\n| Third Language (ENGLISH) | A1       | A2       | A1            | 10          |\\n| Mathematics              | A1       | A1       | A1            | 10          |\\n| Science                  | A1       | A2       | A1            | 09          |\\n| Social Studies           | A1       | A1       | A1            | 10          |\\n| Second Language (HINDI)  | A1       | B1       | B1            | 08          |\\n\\n\\n### **Cumulative Grade Point Average (CGPA): 9.5**\\n\\n\\n### **CO-CURRICULAR AREAS:**'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '### **Cumulative Grade Point Average (CGPA): 9.5**\\n\\n\\n### **CO-CURRICULAR AREAS:**\\n\\n| Subject                       | Grade |\\n| ----------------------------- | ----- |\\n| Value Education & Life Skills | A+    |\\n| Art & Cultural Education      | A+    |\\n| Work & Computer Education     | A+    |\\n| Physical & Health Education   | A+    |\\n\\n\\n**Head Master**\\n**EKALAVYA FOUNDATION SCHOOL**\\n**Nalgonda, Nalgonda**\\n**Date of Issue: 13th May, 2019**\\n\\n**SECRETARY**\\n**Board of Secondary Education**\\n**Telangana State, Hyderabad**\\n\\n---\\n\\n12th class marks(Intermediate)\\n---\\n\\n**Telangana State Board of Intermediate Education**\\nVidya Bhavan, Nampally, Hyderabad - 500 001'}, page_content='### **Cumulative Grade Point Average (CGPA): 9.5**\\n\\n\\n### **CO-CURRICULAR AREAS:**\\n\\n| Subject                       | Grade |\\n| ----------------------------- | ----- |\\n| Value Education & Life Skills | A+    |\\n| Art & Cultural Education      | A+    |\\n| Work & Computer Education     | A+    |\\n| Physical & Health Education   | A+    |\\n\\n\\n**Head Master**\\n**EKALAVYA FOUNDATION SCHOOL**\\n**Nalgonda, Nalgonda**\\n**Date of Issue: 13th May, 2019**\\n\\n**SECRETARY**\\n**Board of Secondary Education**\\n**Telangana State, Hyderabad**\\n\\n---\\n\\n12th class marks(Intermediate)\\n---\\n\\n**Telangana State Board of Intermediate Education**\\nVidya Bhavan, Nampally, Hyderabad - 500 001'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n12th class marks(Intermediate)\\n---\\n\\n**Telangana State Board of Intermediate Education**\\nVidya Bhavan, Nampally, Hyderabad - 500 001\\n\\n**PASS CERTIFICATE-CUM-MEMORANDUM OF MARKS**\\n**This is to certify that**\\n**KATTA SAI PRANAV REDDY**\\n**Father‚Äôs Name:** KATTA SRINIVAS REDDY\\n**Mother‚Äôs Name:** KATTA UMARANI\\n**Registered Number:** 2158208799\\n**Month & Year of Exam:** MARCH 2021\\n**Medium:** ENGLISH\\n**Result:** A GRADE\\nhas appeared for the Intermediate Public Examination held in March 2021 and passed in the following subjects:\\n\\n\\n### **Part - I**\\n\\n**ENGLISH** ‚Äì 98 / 100\\n**SANSKRIT** ‚Äì 100 / 100\\n\\n\\n### **Part - II**\\n\\n**HE** ‚Äì 98 / 100\\n\\n---\\n\\n### **Part - III: Optional Subjects**'}, page_content='---\\n\\n12th class marks(Intermediate)\\n---\\n\\n**Telangana State Board of Intermediate Education**\\nVidya Bhavan, Nampally, Hyderabad - 500 001\\n\\n**PASS CERTIFICATE-CUM-MEMORANDUM OF MARKS**\\n**This is to certify that**\\n**KATTA SAI PRANAV REDDY**\\n**Father‚Äôs Name:** KATTA SRINIVAS REDDY\\n**Mother‚Äôs Name:** KATTA UMARANI\\n**Registered Number:** 2158208799\\n**Month & Year of Exam:** MARCH 2021\\n**Medium:** ENGLISH\\n**Result:** A GRADE\\nhas appeared for the Intermediate Public Examination held in March 2021 and passed in the following subjects:\\n\\n\\n### **Part - I**\\n\\n**ENGLISH** ‚Äì 98 / 100\\n**SANSKRIT** ‚Äì 100 / 100\\n\\n\\n### **Part - II**\\n\\n**HE** ‚Äì 98 / 100\\n\\n---\\n\\n### **Part - III: Optional Subjects**'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '### **Part - I**\\n\\n**ENGLISH** ‚Äì 98 / 100\\n**SANSKRIT** ‚Äì 100 / 100\\n\\n\\n### **Part - II**\\n\\n**HE** ‚Äì 98 / 100\\n\\n---\\n\\n### **Part - III: Optional Subjects**\\n\\n| Subject              | Max Marks | Marks Obtained |\\n| -------------------- | --------- | -------------- |\\n| MATHEMATICS A        | 75        | 75             |\\n| MATHEMATICS B        | 75        | 75             |\\n| PHYSICS              | 60        | 58             |\\n| PHYSICS PRACTICALS   | 30        | 30             |\\n| CHEMISTRY            | 60        | 60             |\\n| CHEMISTRY PRACTICALS | 30        | 30             |\\n\\n---\\n\\n### **Environmental Education:** QUALIFIED\\n\\n### **Ethics and Human Values:** QUALIFIED\\n\\n---\\n\\n**Total Marks:**\\nIn Figures: **982**\\nIn Words: **NINE EIGHT TWO**\\n\\n---\\n\\n**Date:** 28-06-2021\\n**Signature of the Principal and College Seal:** Sri Chaitanya Jr. Kalasala\\n**Signature:** (Controller of Examinations)\\n\\n---'}, page_content='### **Part - I**\\n\\n**ENGLISH** ‚Äì 98 / 100\\n**SANSKRIT** ‚Äì 100 / 100\\n\\n\\n### **Part - II**\\n\\n**HE** ‚Äì 98 / 100\\n\\n---\\n\\n### **Part - III: Optional Subjects**\\n\\n| Subject              | Max Marks | Marks Obtained |\\n| -------------------- | --------- | -------------- |\\n| MATHEMATICS A        | 75        | 75             |\\n| MATHEMATICS B        | 75        | 75             |\\n| PHYSICS              | 60        | 58             |\\n| PHYSICS PRACTICALS   | 30        | 30             |\\n| CHEMISTRY            | 60        | 60             |\\n| CHEMISTRY PRACTICALS | 30        | 30             |\\n\\n---\\n\\n### **Environmental Education:** QUALIFIED\\n\\n### **Ethics and Human Values:** QUALIFIED\\n\\n---\\n\\n**Total Marks:**\\nIn Figures: **982**\\nIn Words: **NINE EIGHT TWO**\\n\\n---\\n\\n**Date:** 28-06-2021\\n**Signature of the Principal and College Seal:** Sri Chaitanya Jr. Kalasala\\n**Signature:** (Controller of Examinations)\\n\\n---'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n**Date:** 28-06-2021\\n**Signature of the Principal and College Seal:** Sri Chaitanya Jr. Kalasala\\n**Signature:** (Controller of Examinations)\\n\\n---\\n\\nResume \\n---\\n**Katta Sai Pranav Reddy**\\nEmail: [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\\nGitHub: ka1817\\nLinkedIn: pranav-reddy-katta\\n\\n---\\n\\n### **Professional Summary**\\n\\nAI and ML Engineer skilled in developing end-to-end machine learning and Generative AI solutions for real-world business challenges. Proficient in data preprocessing, exploratory data analysis, and building predictive models to deliver actionable insights. Experienced in leveraging advanced AI techniques and data-driven strategies to create scalable, impactful solutions.\\n\\n---\\n\\n### **Education**\\n\\n* **Anurag University**, Hyderabad, India\\n  B.Tech in Artificial Intelligence and Machine Learning; CGPA: 8.29\\n  *09/2021 ‚Äì 04/2025*'}, page_content='---\\n\\n**Date:** 28-06-2021\\n**Signature of the Principal and College Seal:** Sri Chaitanya Jr. Kalasala\\n**Signature:** (Controller of Examinations)\\n\\n---\\n\\nResume \\n---\\n**Katta Sai Pranav Reddy**\\nEmail: [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\\nGitHub: ka1817\\nLinkedIn: pranav-reddy-katta\\n\\n---\\n\\n### **Professional Summary**\\n\\nAI and ML Engineer skilled in developing end-to-end machine learning and Generative AI solutions for real-world business challenges. Proficient in data preprocessing, exploratory data analysis, and building predictive models to deliver actionable insights. Experienced in leveraging advanced AI techniques and data-driven strategies to create scalable, impactful solutions.\\n\\n---\\n\\n### **Education**\\n\\n* **Anurag University**, Hyderabad, India\\n  B.Tech in Artificial Intelligence and Machine Learning; CGPA: 8.29\\n  *09/2021 ‚Äì 04/2025*'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n### **Education**\\n\\n* **Anurag University**, Hyderabad, India\\n  B.Tech in Artificial Intelligence and Machine Learning; CGPA: 8.29\\n  *09/2021 ‚Äì 04/2025*\\n\\n* **Sri Chaitanya Junior College**, Hyderabad, India\\n  MPC (Maths, Physics, Chemistry); Percentage: 98%\\n  *06/2019 ‚Äì 05/2021*\\n\\n---\\n\\n### Experience\\n\\n**iNeuron Intelligence Pvt. Ltd.** *(Remote)*\\n*Machine Learning Intern ‚Äî 10/2024 ‚Äì 11/2024*'}, page_content='---\\n\\n### **Education**\\n\\n* **Anurag University**, Hyderabad, India\\n  B.Tech in Artificial Intelligence and Machine Learning; CGPA: 8.29\\n  *09/2021 ‚Äì 04/2025*\\n\\n* **Sri Chaitanya Junior College**, Hyderabad, India\\n  MPC (Maths, Physics, Chemistry); Percentage: 98%\\n  *06/2019 ‚Äì 05/2021*\\n\\n---\\n\\n### Experience\\n\\n**iNeuron Intelligence Pvt. Ltd.** *(Remote)*\\n*Machine Learning Intern ‚Äî 10/2024 ‚Äì 11/2024*'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n### Experience\\n\\n**iNeuron Intelligence Pvt. Ltd.** *(Remote)*\\n*Machine Learning Intern ‚Äî 10/2024 ‚Äì 11/2024*\\n\\n* Conducted extensive data preprocessing and exploratory data analysis (EDA) on large customer datasets to identify key behavioral patterns and high-value customer segments.\\n* Developed and trained machine learning models for customer segmentation using clustering techniques such as K-Means and Hierarchical Clustering, enhancing marketing strategy alignment.\\n* Collaborated with cross-functional teams to interpret analytical insights and monitored model performance across different stages of the pipeline, ensuring accuracy and consistency.\\n* Delivered actionable recommendations based on statistical analysis and predictive modeling, supporting data-driven decision-making for targeted marketing campaigns.\\n\\n**Unified Mentor Pvt. Ltd.** *(Remote)*\\n*Data Science Intern ‚Äî 09/2024 ‚Äì 10/2024*'}, page_content='---\\n\\n### Experience\\n\\n**iNeuron Intelligence Pvt. Ltd.** *(Remote)*\\n*Machine Learning Intern ‚Äî 10/2024 ‚Äì 11/2024*\\n\\n* Conducted extensive data preprocessing and exploratory data analysis (EDA) on large customer datasets to identify key behavioral patterns and high-value customer segments.\\n* Developed and trained machine learning models for customer segmentation using clustering techniques such as K-Means and Hierarchical Clustering, enhancing marketing strategy alignment.\\n* Collaborated with cross-functional teams to interpret analytical insights and monitored model performance across different stages of the pipeline, ensuring accuracy and consistency.\\n* Delivered actionable recommendations based on statistical analysis and predictive modeling, supporting data-driven decision-making for targeted marketing campaigns.\\n\\n**Unified Mentor Pvt. Ltd.** *(Remote)*\\n*Data Science Intern ‚Äî 09/2024 ‚Äì 10/2024*'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '**Unified Mentor Pvt. Ltd.** *(Remote)*\\n*Data Science Intern ‚Äî 09/2024 ‚Äì 10/2024*\\n\\n* Developed and optimized machine learning models to predict employee attrition, enabling proactive retention strategies and improving workforce stability.\\n* Conducted comprehensive data preprocessing, feature engineering, and exploratory data analysis (EDA) to identify key factors influencing employee turnover.\\n* Delivered actionable insights and visualized patterns through dashboards and reports, supporting HR teams in making data-driven decisions.\\n* Presented findings to stakeholders, translating complex analytics into clear, strategic recommendations for reducing attrition risk.\\n\\n---\\n\\n### **Projects**\\n\\n**BigBasket SmartCart ‚Äì AI-Driven Shopping Assistant** *(06/2025 ‚Äì 07/2025)* \\\\[GitHub]'}, page_content='**Unified Mentor Pvt. Ltd.** *(Remote)*\\n*Data Science Intern ‚Äî 09/2024 ‚Äì 10/2024*\\n\\n* Developed and optimized machine learning models to predict employee attrition, enabling proactive retention strategies and improving workforce stability.\\n* Conducted comprehensive data preprocessing, feature engineering, and exploratory data analysis (EDA) to identify key factors influencing employee turnover.\\n* Delivered actionable insights and visualized patterns through dashboards and reports, supporting HR teams in making data-driven decisions.\\n* Presented findings to stakeholders, translating complex analytics into clear, strategic recommendations for reducing attrition risk.\\n\\n---\\n\\n### **Projects**\\n\\n**BigBasket SmartCart ‚Äì AI-Driven Shopping Assistant** *(06/2025 ‚Äì 07/2025)* \\\\[GitHub]'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '* Led development of an AI-driven shopping assistant using RAG, enabling natural language queries and semantic product search with 95% retrieval accuracy for real-time product recommendations.\\n* Developed a retrieval pipeline using the gte-small model, FAISS indexing, and Cross-Encoder reranking, which improved relevance score to 0.89 for intent-driven search results.\\n* Designed a modular architecture with FastAPI, HTML/CSS, and Docker, ensuring scalability and reduced response latency to \\\\~2 seconds for seamless interactions.\\n* Implemented GitHub Actions for automated testing, Docker builds, and AWS EC2 deployment, which reduced deployment time by 40% and improved system reliability.\\nüõí BigBasket SmartCart ‚Äì AI Assistant for BigBasket Shopping\\nüßæ Introduction'}, page_content='* Led development of an AI-driven shopping assistant using RAG, enabling natural language queries and semantic product search with 95% retrieval accuracy for real-time product recommendations.\\n* Developed a retrieval pipeline using the gte-small model, FAISS indexing, and Cross-Encoder reranking, which improved relevance score to 0.89 for intent-driven search results.\\n* Designed a modular architecture with FastAPI, HTML/CSS, and Docker, ensuring scalability and reduced response latency to \\\\~2 seconds for seamless interactions.\\n* Implemented GitHub Actions for automated testing, Docker builds, and AWS EC2 deployment, which reduced deployment time by 40% and improved system reliability.\\nüõí BigBasket SmartCart ‚Äì AI Assistant for BigBasket Shopping\\nüßæ Introduction'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': \"üõí BigBasket SmartCart ‚Äì AI Assistant for BigBasket Shopping\\nüßæ Introduction\\nThe rapid evolution of AI technologies has created new opportunities for enhancing user experience in digital commerce. Leveraging state-of-the-art language models and retrieval systems, intelligent assistants can now understand complex queries, process vast amounts of product data, and deliver precise, context-aware responses. This project presents a scalable and robust AI-powered shopping assistant tailored for BigBasket's product ecosystem. Built using Retrieval-Augmented Generation (RAG), vector embeddings, and large language models (LLMs), the system enables efficient and intelligent product discovery through natural language interaction.\"}, page_content=\"üõí BigBasket SmartCart ‚Äì AI Assistant for BigBasket Shopping\\nüßæ Introduction\\nThe rapid evolution of AI technologies has created new opportunities for enhancing user experience in digital commerce. Leveraging state-of-the-art language models and retrieval systems, intelligent assistants can now understand complex queries, process vast amounts of product data, and deliver precise, context-aware responses. This project presents a scalable and robust AI-powered shopping assistant tailored for BigBasket's product ecosystem. Built using Retrieval-Augmented Generation (RAG), vector embeddings, and large language models (LLMs), the system enables efficient and intelligent product discovery through natural language interaction.\"),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '‚ùó Problem Statement\\nOnline shoppers frequently seek personalized and context-specific product recommendations, such as identifying the best-rated skincare item at the lowest price. However, conventional search systems often fall short in understanding such nuanced queries, lacking the ability to interpret intent, compare attributes across products, and deliver concise, relevant results. This creates friction in the user journey, leading to suboptimal shopping experiences. There is a clear need for an intelligent assistant that can process natural language queries, reason over structured product data, and deliver accurate, insightful responses to aid decision-making.\\n\\nBusiness Goal:\\nTo enhance the shopping experience, boost conversion rates, and optimize search efficiency by enabling natural language-based product search that understands user intent and delivers context-aware, personalized recommendations.\\n\\nüí∞ Business Impact (Revenue + Cost)\\nüí∏ 1. Increased Conversion Rates (‚Üë Revenue)'}, page_content='‚ùó Problem Statement\\nOnline shoppers frequently seek personalized and context-specific product recommendations, such as identifying the best-rated skincare item at the lowest price. However, conventional search systems often fall short in understanding such nuanced queries, lacking the ability to interpret intent, compare attributes across products, and deliver concise, relevant results. This creates friction in the user journey, leading to suboptimal shopping experiences. There is a clear need for an intelligent assistant that can process natural language queries, reason over structured product data, and deliver accurate, insightful responses to aid decision-making.\\n\\nBusiness Goal:\\nTo enhance the shopping experience, boost conversion rates, and optimize search efficiency by enabling natural language-based product search that understands user intent and delivers context-aware, personalized recommendations.\\n\\nüí∞ Business Impact (Revenue + Cost)\\nüí∏ 1. Increased Conversion Rates (‚Üë Revenue)'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üí∞ Business Impact (Revenue + Cost)\\nüí∏ 1. Increased Conversion Rates (‚Üë Revenue)\\n\\n‚Ä¢ Users find relevant products faster, leading to more product views, cart adds, and purchases\\n\\n‚Ä¢ Personalized recommendations match buyer intent better than traditional search\\n\\n‚Ä¢ Better UX = lower drop-off rates\\nüìà Even a 1‚Äì2% uplift in conversions from improved product search can lead to significant revenue gains for a large marketplace like BigBasket.\\n\\nüìâ 2. Reduced Customer Support Queries (‚Üì Cost)\\n\\n‚Ä¢ AI assistant can handle informational and product-related queries\\n\\n‚Ä¢ Reduces manual intervention, live chat support, and email volume\\n\\n‚Ä¢ More self-service = less operational overhead\\n‚è±Ô∏è 3. Reduced Time-to-Purchase (‚Üë Efficiency)\\n\\n‚Ä¢ Customers make faster decisions because the assistant summarizes comparisons (e.g., price vs. rating trade-offs)\\n\\n‚Ä¢ This shortens the purchase journey and increases user satisfaction\\nüß™ 4. Rapid Experimentation & Deployment (‚Üì Dev Costs)'}, page_content='üí∞ Business Impact (Revenue + Cost)\\nüí∏ 1. Increased Conversion Rates (‚Üë Revenue)\\n\\n‚Ä¢ Users find relevant products faster, leading to more product views, cart adds, and purchases\\n\\n‚Ä¢ Personalized recommendations match buyer intent better than traditional search\\n\\n‚Ä¢ Better UX = lower drop-off rates\\nüìà Even a 1‚Äì2% uplift in conversions from improved product search can lead to significant revenue gains for a large marketplace like BigBasket.\\n\\nüìâ 2. Reduced Customer Support Queries (‚Üì Cost)\\n\\n‚Ä¢ AI assistant can handle informational and product-related queries\\n\\n‚Ä¢ Reduces manual intervention, live chat support, and email volume\\n\\n‚Ä¢ More self-service = less operational overhead\\n‚è±Ô∏è 3. Reduced Time-to-Purchase (‚Üë Efficiency)\\n\\n‚Ä¢ Customers make faster decisions because the assistant summarizes comparisons (e.g., price vs. rating trade-offs)\\n\\n‚Ä¢ This shortens the purchase journey and increases user satisfaction\\nüß™ 4. Rapid Experimentation & Deployment (‚Üì Dev Costs)'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '‚Ä¢ This shortens the purchase journey and increases user satisfaction\\nüß™ 4. Rapid Experimentation & Deployment (‚Üì Dev Costs)\\n\\n‚Ä¢ The project is modular, Dockerized, and CI/CD enabled ‚Üí easier to iterate and deploy\\n\\n‚Ä¢ Can be extended to other verticals (electronics, fashion) or other marketplaces with minimal changes\\nüöÄ Features\\nüîç Natural Language Product Search Users can ask queries like \"cheapest skin care with highest rating\" or \"best perfume under ‚Çπ500\".\\n\\nüß† Query Rewriting with LLM Uses Groq LLMs (gemma2-9b-it) to refine user queries for more precise retrieval.\\n\\nüìÑ Document Embedding & Vector Search Preprocessed BigBasket product data embedded with thenlper/gte-small and indexed using FAISS.\\n\\nü§ñ RAG Pipeline Uses llama3-70b-8192 model for final answer generation based on retrieved and reranked results.\\n\\nüîÅ Reranking with CrossEncoder Improves accuracy using cross-encoder/ms-marco-MiniLM-L-6-v2.\\n\\nüåê FastAPI Backend Easily accessible via localhost:8000 or deployed server.'}, page_content='‚Ä¢ This shortens the purchase journey and increases user satisfaction\\nüß™ 4. Rapid Experimentation & Deployment (‚Üì Dev Costs)\\n\\n‚Ä¢ The project is modular, Dockerized, and CI/CD enabled ‚Üí easier to iterate and deploy\\n\\n‚Ä¢ Can be extended to other verticals (electronics, fashion) or other marketplaces with minimal changes\\nüöÄ Features\\nüîç Natural Language Product Search Users can ask queries like \"cheapest skin care with highest rating\" or \"best perfume under ‚Çπ500\".\\n\\nüß† Query Rewriting with LLM Uses Groq LLMs (gemma2-9b-it) to refine user queries for more precise retrieval.\\n\\nüìÑ Document Embedding & Vector Search Preprocessed BigBasket product data embedded with thenlper/gte-small and indexed using FAISS.\\n\\nü§ñ RAG Pipeline Uses llama3-70b-8192 model for final answer generation based on retrieved and reranked results.\\n\\nüîÅ Reranking with CrossEncoder Improves accuracy using cross-encoder/ms-marco-MiniLM-L-6-v2.\\n\\nüåê FastAPI Backend Easily accessible via localhost:8000 or deployed server.'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üîÅ Reranking with CrossEncoder Improves accuracy using cross-encoder/ms-marco-MiniLM-L-6-v2.\\n\\nüåê FastAPI Backend Easily accessible via localhost:8000 or deployed server.\\n\\nüê≥ Dockerized Build once, run anywhere. Fully containerized using Docker.\\n\\nüö∞ CI/CD with GitHub Actions Automated testing, image build, and push to DockerHub.\\n\\nüìú Logging Logging implemented for each step in the pipeline for transparency and debugging.'}, page_content='üîÅ Reranking with CrossEncoder Improves accuracy using cross-encoder/ms-marco-MiniLM-L-6-v2.\\n\\nüåê FastAPI Backend Easily accessible via localhost:8000 or deployed server.\\n\\nüê≥ Dockerized Build once, run anywhere. Fully containerized using Docker.\\n\\nüö∞ CI/CD with GitHub Actions Automated testing, image build, and push to DockerHub.\\n\\nüìú Logging Logging implemented for each step in the pipeline for transparency and debugging.'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üö∞ CI/CD with GitHub Actions Automated testing, image build, and push to DockerHub.\\n\\nüìú Logging Logging implemented for each step in the pipeline for transparency and debugging.\\n\\nüóÇÔ∏è Folder Structure\\nBIGBASKET/\\n‚îú‚îÄ‚îÄ .github/\\n‚îÇ   ‚îî‚îÄ‚îÄ workflows/\\n‚îÇ       ‚îî‚îÄ‚îÄ ci-cd.yml\\n‚îú‚îÄ‚îÄ data/\\n‚îÇ   ‚îî‚îÄ‚îÄ BigBasket Products.csv\\n‚îú‚îÄ‚îÄ logs/\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.log\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.log\\n‚îÇ   ‚îú‚îÄ‚îÄ query_rewriting.log\\n‚îÇ   ‚îî‚îÄ‚îÄ retrieval_generation.log\\n‚îú‚îÄ‚îÄ src/\\n‚îÇ   ‚îú‚îÄ‚îÄ utils/\\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py\\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py\\n‚îÇ   ‚îú‚îÄ‚îÄ query_rewritting.py\\n‚îÇ   ‚îî‚îÄ‚îÄ retrival_genaration.py\\n‚îú‚îÄ‚îÄ static/\\n‚îÇ   ‚îî‚îÄ‚îÄ css/\\n‚îÇ       ‚îî‚îÄ‚îÄ style.css\\n‚îú‚îÄ‚îÄ templates/\\n‚îÇ   ‚îî‚îÄ‚îÄ index.html\\n‚îú‚îÄ‚îÄ tests/\\n‚îú‚îÄ‚îÄ ui/\\n‚îú‚îÄ‚îÄ main.py\\n‚îú‚îÄ‚îÄ Dockerfile\\n‚îú‚îÄ‚îÄ requirements.txt\\n‚îú‚îÄ‚îÄ .env\\n‚îú‚îÄ‚îÄ .dockerignore\\n‚îú‚îÄ‚îÄ .gitignore\\n‚îî‚îÄ‚îÄ README.md\\nüß™ Local Development Setup\\n# Clone the repository\\ngit clone https://github.com/ka1817/BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\ncd BigBasket'}, page_content='üö∞ CI/CD with GitHub Actions Automated testing, image build, and push to DockerHub.\\n\\nüìú Logging Logging implemented for each step in the pipeline for transparency and debugging.\\n\\nüóÇÔ∏è Folder Structure\\nBIGBASKET/\\n‚îú‚îÄ‚îÄ .github/\\n‚îÇ   ‚îî‚îÄ‚îÄ workflows/\\n‚îÇ       ‚îî‚îÄ‚îÄ ci-cd.yml\\n‚îú‚îÄ‚îÄ data/\\n‚îÇ   ‚îî‚îÄ‚îÄ BigBasket Products.csv\\n‚îú‚îÄ‚îÄ logs/\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.log\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.log\\n‚îÇ   ‚îú‚îÄ‚îÄ query_rewriting.log\\n‚îÇ   ‚îî‚îÄ‚îÄ retrieval_generation.log\\n‚îú‚îÄ‚îÄ src/\\n‚îÇ   ‚îú‚îÄ‚îÄ utils/\\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py\\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py\\n‚îÇ   ‚îú‚îÄ‚îÄ query_rewritting.py\\n‚îÇ   ‚îî‚îÄ‚îÄ retrival_genaration.py\\n‚îú‚îÄ‚îÄ static/\\n‚îÇ   ‚îî‚îÄ‚îÄ css/\\n‚îÇ       ‚îî‚îÄ‚îÄ style.css\\n‚îú‚îÄ‚îÄ templates/\\n‚îÇ   ‚îî‚îÄ‚îÄ index.html\\n‚îú‚îÄ‚îÄ tests/\\n‚îú‚îÄ‚îÄ ui/\\n‚îú‚îÄ‚îÄ main.py\\n‚îú‚îÄ‚îÄ Dockerfile\\n‚îú‚îÄ‚îÄ requirements.txt\\n‚îú‚îÄ‚îÄ .env\\n‚îú‚îÄ‚îÄ .dockerignore\\n‚îú‚îÄ‚îÄ .gitignore\\n‚îî‚îÄ‚îÄ README.md\\nüß™ Local Development Setup\\n# Clone the repository\\ngit clone https://github.com/ka1817/BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\ncd BigBasket'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '# Create virtual environment\\npython -m venv venv\\nsource venv/bin/activate   # On Windows: venv\\\\Scripts\\\\activate\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Before Running the app set .env(environment variable GROQ_API_KEY)\\nuvicorn main:app --reload --port 8000\\nüê≥ Docker Instructions\\nüîß 1. Pull Image\\n\\ndocker pull pranavreddy123/bigbasket-assistant:latest\\nüöÄ 2. Run the App (Detached Mode)\\n\\ndocker run -d -p 8000:8000 \\\\\\n-e GROQ_API_KEY=create groq api from groq cloud \\\\\\npranavreddy123/bigbasket-assistant:latest\\nüåê 3. Access the App\\n\\nhttp://localhost:8000\\nü§ñ Example Usage\\n\\nQuery: \"Which is the cheapest hair product with high rating?\" Rewritten: \"Find the most affordable hair care product with a high customer rating.\" Response: \"Garlic Oil - Vegetarian Capsule 500 mg by Sri Sri Ayurveda is available at ‚Çπ220 with a 4.1 rating.\"\\n\\nüõ†Ô∏è GitHub Actions (CI/CD)\\nFile: .github/workflows/ci-cd.yml\\n\\n‚úÖ CI-Test: Runs unit tests using pytest.'}, page_content='# Create virtual environment\\npython -m venv venv\\nsource venv/bin/activate   # On Windows: venv\\\\Scripts\\\\activate\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Before Running the app set .env(environment variable GROQ_API_KEY)\\nuvicorn main:app --reload --port 8000\\nüê≥ Docker Instructions\\nüîß 1. Pull Image\\n\\ndocker pull pranavreddy123/bigbasket-assistant:latest\\nüöÄ 2. Run the App (Detached Mode)\\n\\ndocker run -d -p 8000:8000 \\\\\\n-e GROQ_API_KEY=create groq api from groq cloud \\\\\\npranavreddy123/bigbasket-assistant:latest\\nüåê 3. Access the App\\n\\nhttp://localhost:8000\\nü§ñ Example Usage\\n\\nQuery: \"Which is the cheapest hair product with high rating?\" Rewritten: \"Find the most affordable hair care product with a high customer rating.\" Response: \"Garlic Oil - Vegetarian Capsule 500 mg by Sri Sri Ayurveda is available at ‚Çπ220 with a 4.1 rating.\"\\n\\nüõ†Ô∏è GitHub Actions (CI/CD)\\nFile: .github/workflows/ci-cd.yml\\n\\n‚úÖ CI-Test: Runs unit tests using pytest.'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üõ†Ô∏è GitHub Actions (CI/CD)\\nFile: .github/workflows/ci-cd.yml\\n\\n‚úÖ CI-Test: Runs unit tests using pytest.\\n\\nüê≥ CD-Docker: Builds Docker image and pushes to DockerHub.\\n\\nTriggered on push to main or pull request.\\n\\n‚òÅÔ∏è Deployment on Amazon EC2\\n1. Launch EC2 Instance (Ubuntu 20.04)\\n2. SSH into your instance\\nssh -i \"your-key.pem\" ubuntu@your-ec2-ip\\n3. Install Docker\\nsudo apt update\\nsudo apt install docker.io -y\\nsudo systemctl start docker\\nsudo systemctl enable docker\\n4. Pull and Run Docker Image\\ndocker pull pranavreddy123/bigbasket-assistant:latest\\n# Ensure your .env file is in the same directory, or create an API key using Groq Cloud and add it to the .env file\\ndocker run -d --env-file .env -p 8000:8000 pranavreddy123/bigbasket-assistant:latest\\nAccess your app via http://<your-ec2-public-ip>\\nüß† Tech Stack\\n‚úÖ LLMs: Groq (gemma2-9b-it, llama3-70b-8192)\\n\\n‚úÖ LangChain, FAISS, HuggingFace, CrossEncoder\\n\\n‚úÖ FastAPI\\n\\n‚úÖ Docker\\n\\n‚úÖ GitHub Actions\\n\\n‚úÖ AWS EC2\\n\\n‚úÖ HTML/CSS'}, page_content='üõ†Ô∏è GitHub Actions (CI/CD)\\nFile: .github/workflows/ci-cd.yml\\n\\n‚úÖ CI-Test: Runs unit tests using pytest.\\n\\nüê≥ CD-Docker: Builds Docker image and pushes to DockerHub.\\n\\nTriggered on push to main or pull request.\\n\\n‚òÅÔ∏è Deployment on Amazon EC2\\n1. Launch EC2 Instance (Ubuntu 20.04)\\n2. SSH into your instance\\nssh -i \"your-key.pem\" ubuntu@your-ec2-ip\\n3. Install Docker\\nsudo apt update\\nsudo apt install docker.io -y\\nsudo systemctl start docker\\nsudo systemctl enable docker\\n4. Pull and Run Docker Image\\ndocker pull pranavreddy123/bigbasket-assistant:latest\\n# Ensure your .env file is in the same directory, or create an API key using Groq Cloud and add it to the .env file\\ndocker run -d --env-file .env -p 8000:8000 pranavreddy123/bigbasket-assistant:latest\\nAccess your app via http://<your-ec2-public-ip>\\nüß† Tech Stack\\n‚úÖ LLMs: Groq (gemma2-9b-it, llama3-70b-8192)\\n\\n‚úÖ LangChain, FAISS, HuggingFace, CrossEncoder\\n\\n‚úÖ FastAPI\\n\\n‚úÖ Docker\\n\\n‚úÖ GitHub Actions\\n\\n‚úÖ AWS EC2\\n\\n‚úÖ HTML/CSS'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '‚úÖ LangChain, FAISS, HuggingFace, CrossEncoder\\n\\n‚úÖ FastAPI\\n\\n‚úÖ Docker\\n\\n‚úÖ GitHub Actions\\n\\n‚úÖ AWS EC2\\n\\n‚úÖ HTML/CSS\\n\\nüîó Links\\nüîç GitHub Repo: BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\n\\nüê≥ DockerHub: pranavreddy123/bigbasket-assistant\\n\\nüßë\\u200düíª Developed By\\nPranav Reddy\\n\\n**Netflix Customer Churn Prediction ‚Äì End-to-End ML System** *(Personal Project)* \\\\[GitHub]'}, page_content='‚úÖ LangChain, FAISS, HuggingFace, CrossEncoder\\n\\n‚úÖ FastAPI\\n\\n‚úÖ Docker\\n\\n‚úÖ GitHub Actions\\n\\n‚úÖ AWS EC2\\n\\n‚úÖ HTML/CSS\\n\\nüîó Links\\nüîç GitHub Repo: BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\n\\nüê≥ DockerHub: pranavreddy123/bigbasket-assistant\\n\\nüßë\\u200düíª Developed By\\nPranav Reddy\\n\\n**Netflix Customer Churn Prediction ‚Äì End-to-End ML System** *(Personal Project)* \\\\[GitHub]'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üê≥ DockerHub: pranavreddy123/bigbasket-assistant\\n\\nüßë\\u200düíª Developed By\\nPranav Reddy\\n\\n**Netflix Customer Churn Prediction ‚Äì End-to-End ML System** *(Personal Project)* \\\\[GitHub]\\n\\n* Developed a complete machine learning pipeline to predict customer churn, achieving 99% recall and 0.99 ROC AUC through feature engineering, hyperparameter tuning, and cross-validation.\\n* Performed in-depth EDA to identify key churn drivers such as low engagement, infrequent logins, and payment methods, improving model interpretability and business insights.\\n* Implemented reproducible MLOps workflows with data versioning using DVC and AWS S3, and tracked experiments, metrics, and model artifacts using MLflow.\\n* Designed and deployed a FastAPI-based REST API with HTML/CSS frontend for real-time predictions, containerized the application using Docker, and automated CI/CD using GitHub Actions for deployment on AWS EC2.'}, page_content='üê≥ DockerHub: pranavreddy123/bigbasket-assistant\\n\\nüßë\\u200düíª Developed By\\nPranav Reddy\\n\\n**Netflix Customer Churn Prediction ‚Äì End-to-End ML System** *(Personal Project)* \\\\[GitHub]\\n\\n* Developed a complete machine learning pipeline to predict customer churn, achieving 99% recall and 0.99 ROC AUC through feature engineering, hyperparameter tuning, and cross-validation.\\n* Performed in-depth EDA to identify key churn drivers such as low engagement, infrequent logins, and payment methods, improving model interpretability and business insights.\\n* Implemented reproducible MLOps workflows with data versioning using DVC and AWS S3, and tracked experiments, metrics, and model artifacts using MLflow.\\n* Designed and deployed a FastAPI-based REST API with HTML/CSS frontend for real-time predictions, containerized the application using Docker, and automated CI/CD using GitHub Actions for deployment on AWS EC2.'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üìä Netflix Customer Churn Prediction\\nüöÄ Project Overview\\nNetflix, like many subscription-based platforms, faces the challenge of customer churn. Retaining existing customers is significantly more cost-effective than acquiring new ones. This project delivers a full-scale machine learning solution to predict customer churn using behavioral and subscription data, from ingestion to deployment via a FastAPI interface.\\n\\nThis repository presents a production-grade, explainable, and reproducible ML pipeline with CI/CD, experiment tracking (MLflow), data versioning (DVC), and containerized deployment using Docker.\\n\\nüéØ Problem Statement\\nNetflix seeks to proactively identify users likely to cancel their subscriptions. Predicting churn enables targeted interventions to retain users and minimize revenue loss.\\n\\nGoal: Build an ML classification model that predicts churn based on customer behavior and plan details.'}, page_content='üìä Netflix Customer Churn Prediction\\nüöÄ Project Overview\\nNetflix, like many subscription-based platforms, faces the challenge of customer churn. Retaining existing customers is significantly more cost-effective than acquiring new ones. This project delivers a full-scale machine learning solution to predict customer churn using behavioral and subscription data, from ingestion to deployment via a FastAPI interface.\\n\\nThis repository presents a production-grade, explainable, and reproducible ML pipeline with CI/CD, experiment tracking (MLflow), data versioning (DVC), and containerized deployment using Docker.\\n\\nüéØ Problem Statement\\nNetflix seeks to proactively identify users likely to cancel their subscriptions. Predicting churn enables targeted interventions to retain users and minimize revenue loss.\\n\\nGoal: Build an ML classification model that predicts churn based on customer behavior and plan details.'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üìå Key Features Used\\nFeature\\tType\\tDescription\\nwatch_hours\\tNumerical\\tTotal hours watched\\nlast_login_days\\tNumerical\\tDays since last login\\nnumber_of_profiles\\tNumerical\\tTotal profiles under the account\\navg_watch_time_per_day\\tNumerical\\tDaily average watch time\\nsubscription_type\\tCategorical\\tSubscription level: Basic, Standard, Premium\\npayment_method\\tCategorical\\tPayment method: Credit Card, UPI, PayPal, etc.\\nchurned\\tTarget\\t1 = Churned, 0 = Not churned\\nüìä Key EDA Insights\\nüî¨ Feature Significance\\nFeature\\tTest\\tp-value\\tSignificant?\\nsubscription_type\\tChi-Square\\t0.0000\\t‚úÖ Yes\\npayment_method\\tChi-Square\\t0.0000\\t‚úÖ Yes\\nnumber_of_profiles\\tChi-Square\\t0.0000\\t‚úÖ Yes\\nwatch_hours\\tMann-Whitney U\\t0.0000\\t‚úÖ Yes\\nlast_login_days\\tMann-Whitney U\\t0.0000\\t‚úÖ Yes\\navg_watch_time_per_day\\tMann-Whitney U\\t0.0000\\t‚úÖ Yes\\nage\\tMann-Whitney U\\t0.7803\\t‚ùå No\\ngender, region, device\\tChi-Square\\t> 0.3\\t‚ùå No\\n‚úÖ These statistically significant features were included in the final model pipeline.'}, page_content='üìå Key Features Used\\nFeature\\tType\\tDescription\\nwatch_hours\\tNumerical\\tTotal hours watched\\nlast_login_days\\tNumerical\\tDays since last login\\nnumber_of_profiles\\tNumerical\\tTotal profiles under the account\\navg_watch_time_per_day\\tNumerical\\tDaily average watch time\\nsubscription_type\\tCategorical\\tSubscription level: Basic, Standard, Premium\\npayment_method\\tCategorical\\tPayment method: Credit Card, UPI, PayPal, etc.\\nchurned\\tTarget\\t1 = Churned, 0 = Not churned\\nüìä Key EDA Insights\\nüî¨ Feature Significance\\nFeature\\tTest\\tp-value\\tSignificant?\\nsubscription_type\\tChi-Square\\t0.0000\\t‚úÖ Yes\\npayment_method\\tChi-Square\\t0.0000\\t‚úÖ Yes\\nnumber_of_profiles\\tChi-Square\\t0.0000\\t‚úÖ Yes\\nwatch_hours\\tMann-Whitney U\\t0.0000\\t‚úÖ Yes\\nlast_login_days\\tMann-Whitney U\\t0.0000\\t‚úÖ Yes\\navg_watch_time_per_day\\tMann-Whitney U\\t0.0000\\t‚úÖ Yes\\nage\\tMann-Whitney U\\t0.7803\\t‚ùå No\\ngender, region, device\\tChi-Square\\t> 0.3\\t‚ùå No\\n‚úÖ These statistically significant features were included in the final model pipeline.'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üèóÔ∏è Project Architecture\\nnetflix-churn-prediction/\\n‚îú‚îÄ‚îÄ data/                     # Raw and processed data\\n‚îú‚îÄ‚îÄ models/                   # Trained model binaries\\n‚îú‚îÄ‚îÄ reports/                  # Classification reports & plots\\n‚îú‚îÄ‚îÄ static/                   # CSS\\n‚îú‚îÄ‚îÄ templates/                # HTML UI\\n‚îú‚îÄ‚îÄ src/\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py     # Load dataset\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py # Pipeline for scaling & encoding\\n‚îÇ   ‚îî‚îÄ‚îÄ model_training.py     # ML training & evaluation\\n‚îú‚îÄ‚îÄ main.py                   # FastAPI backend\\n‚îú‚îÄ‚îÄ Dockerfile                # Containerization\\n‚îú‚îÄ‚îÄ .dvc/                     # DVC for data version control\\n‚îú‚îÄ‚îÄ .github/workflows/        # CI/CD GitHub Actions\\n‚îî‚îÄ‚îÄ README.md\\n‚öôÔ∏è End-to-End ML Workflow\\n1Ô∏è‚É£ Data Ingestion\\nLoads .csv into DataFrame\\nHandles errors and logs shape/summary\\n2Ô∏è‚É£ Preprocessing\\nOneHotEncoding (categorical)\\nStandardScaler (numerical)\\nUses ColumnTransformer for pipeline modularity\\n3Ô∏è‚É£ Model Training\\nModels: RandomForest, GradientBoosting, SVC'}, page_content='üèóÔ∏è Project Architecture\\nnetflix-churn-prediction/\\n‚îú‚îÄ‚îÄ data/                     # Raw and processed data\\n‚îú‚îÄ‚îÄ models/                   # Trained model binaries\\n‚îú‚îÄ‚îÄ reports/                  # Classification reports & plots\\n‚îú‚îÄ‚îÄ static/                   # CSS\\n‚îú‚îÄ‚îÄ templates/                # HTML UI\\n‚îú‚îÄ‚îÄ src/\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py     # Load dataset\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py # Pipeline for scaling & encoding\\n‚îÇ   ‚îî‚îÄ‚îÄ model_training.py     # ML training & evaluation\\n‚îú‚îÄ‚îÄ main.py                   # FastAPI backend\\n‚îú‚îÄ‚îÄ Dockerfile                # Containerization\\n‚îú‚îÄ‚îÄ .dvc/                     # DVC for data version control\\n‚îú‚îÄ‚îÄ .github/workflows/        # CI/CD GitHub Actions\\n‚îî‚îÄ‚îÄ README.md\\n‚öôÔ∏è End-to-End ML Workflow\\n1Ô∏è‚É£ Data Ingestion\\nLoads .csv into DataFrame\\nHandles errors and logs shape/summary\\n2Ô∏è‚É£ Preprocessing\\nOneHotEncoding (categorical)\\nStandardScaler (numerical)\\nUses ColumnTransformer for pipeline modularity\\n3Ô∏è‚É£ Model Training\\nModels: RandomForest, GradientBoosting, SVC'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '2Ô∏è‚É£ Preprocessing\\nOneHotEncoding (categorical)\\nStandardScaler (numerical)\\nUses ColumnTransformer for pipeline modularity\\n3Ô∏è‚É£ Model Training\\nModels: RandomForest, GradientBoosting, SVC\\nGridSearchCV for hyperparameter tuning\\nModel artifacts saved to models/\\nROC curves + classification reports saved to reports/\\n4Ô∏è‚É£ MLflow Tracking ‚úÖ\\nTracks experiment metadata, metrics, parameters\\nStores models and artifacts\\nUI accessible at localhost:5000\\nüß™ Model Performance\\nModel\\tAccuracy\\tF1 Score\\tROC AUC (Test)\\tROC AUC (CV)\\tNotes\\nRandom Forest\\t0.99\\t0.99\\t0.9995\\t0.9987\\t‚úÖ Best overall„Äê13‚Ä†source„Äë\\nGradient Boosting\\t0.99\\t0.99\\t0.9989\\t0.9991\\tRobust & efficient„Äê12‚Ä†source„Äë\\nSVC\\t0.93\\t0.93\\t0.9844\\t0.9822\\tLightweight„Äê14‚Ä†source„Äë\\nüåê FastAPI Deployment\\nüîß API Endpoints:\\n/: HTML frontend form for manual input\\n/api/predict: JSON-based API for programmatic inference\\nüîå Model Used:\\nRandom Forest (best AUC + accuracy)\\nAccepts form or JSON input\\nReturns churn prediction + confidence\\nüê≥ Docker Setup\\nFROM python:3.10-slim'}, page_content='2Ô∏è‚É£ Preprocessing\\nOneHotEncoding (categorical)\\nStandardScaler (numerical)\\nUses ColumnTransformer for pipeline modularity\\n3Ô∏è‚É£ Model Training\\nModels: RandomForest, GradientBoosting, SVC\\nGridSearchCV for hyperparameter tuning\\nModel artifacts saved to models/\\nROC curves + classification reports saved to reports/\\n4Ô∏è‚É£ MLflow Tracking ‚úÖ\\nTracks experiment metadata, metrics, parameters\\nStores models and artifacts\\nUI accessible at localhost:5000\\nüß™ Model Performance\\nModel\\tAccuracy\\tF1 Score\\tROC AUC (Test)\\tROC AUC (CV)\\tNotes\\nRandom Forest\\t0.99\\t0.99\\t0.9995\\t0.9987\\t‚úÖ Best overall„Äê13‚Ä†source„Äë\\nGradient Boosting\\t0.99\\t0.99\\t0.9989\\t0.9991\\tRobust & efficient„Äê12‚Ä†source„Äë\\nSVC\\t0.93\\t0.93\\t0.9844\\t0.9822\\tLightweight„Äê14‚Ä†source„Äë\\nüåê FastAPI Deployment\\nüîß API Endpoints:\\n/: HTML frontend form for manual input\\n/api/predict: JSON-based API for programmatic inference\\nüîå Model Used:\\nRandom Forest (best AUC + accuracy)\\nAccepts form or JSON input\\nReturns churn prediction + confidence\\nüê≥ Docker Setup\\nFROM python:3.10-slim'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üîå Model Used:\\nRandom Forest (best AUC + accuracy)\\nAccepts form or JSON input\\nReturns churn prediction + confidence\\nüê≥ Docker Setup\\nFROM python:3.10-slim\\nWORKDIR /app\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\nCOPY . .\\nEXPOSE 8000\\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\\nRun locally:'}, page_content='üîå Model Used:\\nRandom Forest (best AUC + accuracy)\\nAccepts form or JSON input\\nReturns churn prediction + confidence\\nüê≥ Docker Setup\\nFROM python:3.10-slim\\nWORKDIR /app\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\nCOPY . .\\nEXPOSE 8000\\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\\nRun locally:'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'docker build -t netflix-churn .\\ndocker run -p 8000:8000 netflix-churn\\nüîÅ CI/CD Pipeline (GitHub Actions)\\n‚úÖ Stages:\\nTest Phase\\n\\nInstall dependencies\\nRun pytest on unit tests\\nPull versioned data using dvc pull\\nBuild Phase\\n\\nDocker image build with CACHEBUST arg\\nPush to DockerHub using GitHub Secrets\\nDeploy Phase'}, page_content='docker build -t netflix-churn .\\ndocker run -p 8000:8000 netflix-churn\\nüîÅ CI/CD Pipeline (GitHub Actions)\\n‚úÖ Stages:\\nTest Phase\\n\\nInstall dependencies\\nRun pytest on unit tests\\nPull versioned data using dvc pull\\nBuild Phase\\n\\nDocker image build with CACHEBUST arg\\nPush to DockerHub using GitHub Secrets\\nDeploy Phase'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'Install dependencies\\nRun pytest on unit tests\\nPull versioned data using dvc pull\\nBuild Phase\\n\\nDocker image build with CACHEBUST arg\\nPush to DockerHub using GitHub Secrets\\nDeploy Phase\\n\\nSSH into EC2 instance\\nStop, remove old container\\nPull and launch updated Docker image\\nüîê GitHub Repository Secrets\\nName\\tPurpose\\nAWS_ACCESS_KEY_ID\\tAWS auth for DVC S3\\nAWS_SECRET_ACCESS_KEY\\tAWS auth for DVC S3\\nDOCKER_USERNAME\\tDockerHub username for push\\nDOCKER_PASSWORD\\tDockerHub password/token\\nEC2_HOST\\tPublic IP/DNS of EC2 instance\\nEC2_USER\\tSSH user for EC2 login\\nEC2_SSH_KEY\\tPrivate SSH key for GitHub Actions\\nüß¨ Data Versioning with DVC\\nTracks raw and preprocessed data versions\\nUses .dvc/config to connect to AWS S3 remote\\nRun dvc push and dvc pull to sync across environments\\nEnsures reproducibility in CI and local experiments\\nüìå Business Value & Insights\\nüß† High-risk churn users are linked to:\\n\\nLow engagement (low watch hours)\\nInfrequent logins\\nBasic plans & non-card payments\\nüìà Operational Benefits:'}, page_content='Install dependencies\\nRun pytest on unit tests\\nPull versioned data using dvc pull\\nBuild Phase\\n\\nDocker image build with CACHEBUST arg\\nPush to DockerHub using GitHub Secrets\\nDeploy Phase\\n\\nSSH into EC2 instance\\nStop, remove old container\\nPull and launch updated Docker image\\nüîê GitHub Repository Secrets\\nName\\tPurpose\\nAWS_ACCESS_KEY_ID\\tAWS auth for DVC S3\\nAWS_SECRET_ACCESS_KEY\\tAWS auth for DVC S3\\nDOCKER_USERNAME\\tDockerHub username for push\\nDOCKER_PASSWORD\\tDockerHub password/token\\nEC2_HOST\\tPublic IP/DNS of EC2 instance\\nEC2_USER\\tSSH user for EC2 login\\nEC2_SSH_KEY\\tPrivate SSH key for GitHub Actions\\nüß¨ Data Versioning with DVC\\nTracks raw and preprocessed data versions\\nUses .dvc/config to connect to AWS S3 remote\\nRun dvc push and dvc pull to sync across environments\\nEnsures reproducibility in CI and local experiments\\nüìå Business Value & Insights\\nüß† High-risk churn users are linked to:\\n\\nLow engagement (low watch hours)\\nInfrequent logins\\nBasic plans & non-card payments\\nüìà Operational Benefits:'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'Preemptive retention campaigns\\nPersonalized offers to vulnerable users\\nReduce marketing costs via targeted outreach\\n‚úÖ Run Locally (No Docker)\\ngit clone <repo_url>\\ncd netflix-churn-prediction\\npython src/model_training.py        # Train all models\\nuvicorn main:app --reload           # Launch API server\\nSummary\\nComponent\\tImplemented\\tTool/Service Used\\nData Versioning\\t‚úÖ\\tDVC with AWS S3 remote\\nData Ingestion\\t‚úÖ\\tpandas, custom Python class\\nData Preprocessing\\t‚úÖ\\tscikit-learn Pipelines\\nModel Training\\t‚úÖ\\tscikit-learn, GridSearchCV\\nExperiment Tracking\\t‚úÖ\\tMLflow (local server: 127.0.0.1:5000)\\nModel Evaluation\\t‚úÖ\\tclassification_report, ROC AUC\\nModel Packaging\\t‚úÖ\\tjoblib for serialization\\nAPI Deployment\\t‚úÖ\\tFastAPI on AWS EC2\\nWeb UI\\t‚úÖ\\tHTML + Bootstrap via Jinja2\\nContainerization\\t‚úÖ\\tDocker (with Dockerfile)\\nCI/CD Pipeline\\t‚úÖ\\tGitHub Actions\\nCloud Hosting\\t‚úÖ\\tAWS EC2, SSH-based deployment\\nSecrets Management\\t‚úÖ\\tGitHub Secrets\\nTesting\\t‚úÖ\\tpytest, CI-tested\\nüôå Author\\nüë®\\u200düíª Katta Sai Pranav Reddy\\nüìé Tech Stack\\nPython 3.10'}, page_content='Preemptive retention campaigns\\nPersonalized offers to vulnerable users\\nReduce marketing costs via targeted outreach\\n‚úÖ Run Locally (No Docker)\\ngit clone <repo_url>\\ncd netflix-churn-prediction\\npython src/model_training.py        # Train all models\\nuvicorn main:app --reload           # Launch API server\\nSummary\\nComponent\\tImplemented\\tTool/Service Used\\nData Versioning\\t‚úÖ\\tDVC with AWS S3 remote\\nData Ingestion\\t‚úÖ\\tpandas, custom Python class\\nData Preprocessing\\t‚úÖ\\tscikit-learn Pipelines\\nModel Training\\t‚úÖ\\tscikit-learn, GridSearchCV\\nExperiment Tracking\\t‚úÖ\\tMLflow (local server: 127.0.0.1:5000)\\nModel Evaluation\\t‚úÖ\\tclassification_report, ROC AUC\\nModel Packaging\\t‚úÖ\\tjoblib for serialization\\nAPI Deployment\\t‚úÖ\\tFastAPI on AWS EC2\\nWeb UI\\t‚úÖ\\tHTML + Bootstrap via Jinja2\\nContainerization\\t‚úÖ\\tDocker (with Dockerfile)\\nCI/CD Pipeline\\t‚úÖ\\tGitHub Actions\\nCloud Hosting\\t‚úÖ\\tAWS EC2, SSH-based deployment\\nSecrets Management\\t‚úÖ\\tGitHub Secrets\\nTesting\\t‚úÖ\\tpytest, CI-tested\\nüôå Author\\nüë®\\u200düíª Katta Sai Pranav Reddy\\nüìé Tech Stack\\nPython 3.10'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'Cloud Hosting\\t‚úÖ\\tAWS EC2, SSH-based deployment\\nSecrets Management\\t‚úÖ\\tGitHub Secrets\\nTesting\\t‚úÖ\\tpytest, CI-tested\\nüôå Author\\nüë®\\u200düíª Katta Sai Pranav Reddy\\nüìé Tech Stack\\nPython 3.10\\nScikit-learn, MLflow, DVC, FastAPI, Docker\\nGitHub Actions, AWS EC2, S3 Remote Storage\\n---'}, page_content='Cloud Hosting\\t‚úÖ\\tAWS EC2, SSH-based deployment\\nSecrets Management\\t‚úÖ\\tGitHub Secrets\\nTesting\\t‚úÖ\\tpytest, CI-tested\\nüôå Author\\nüë®\\u200düíª Katta Sai Pranav Reddy\\nüìé Tech Stack\\nPython 3.10\\nScikit-learn, MLflow, DVC, FastAPI, Docker\\nGitHub Actions, AWS EC2, S3 Remote Storage\\n---'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '### **Skills**\\n\\n* **Tools:** MLflow, DVC, Docker, Git, GitHub Actions, AWS (EC2, S3, ECR), FAISS, Pinecone, Hugging Face, LangChain, LangSmith, FastAPI\\n* **Programming & Technical Skills:** Python, SQL, HTML, CSS, Scikit-learn, TensorFlow, Keras, Statistics\\n* **Data Science & Machine Learning:** Data Preprocessing, EDA, Feature Engineering, Model Training & Evaluation, Hyperparameter Tuning, Clustering, MLOps, Semantic Search, Retrieval-Augmented Generation (RAG), CNN, RNN, GPT, Transformers, Fine-Tuning, Prompt Engineering\\n* **Data Visualization & Analysis:** Pandas, NumPy, Matplotlib, Seaborn\\n\\n---\\n\\nhobbies section \\n\\n---\\n\\n### **Hobbies & Interests**\\nHobbies & Interests\\n\\n* Playing Cricket\\n* Watching Football\\n* Reading Books\\n* Exploring Latest Advancements in Artificial Intelligence\\n* Browsing the Internet for Tech & Knowledge Updates\\n\\n---\\n\\n### Contact Information\\n\\nContact Information'}, page_content='### **Skills**\\n\\n* **Tools:** MLflow, DVC, Docker, Git, GitHub Actions, AWS (EC2, S3, ECR), FAISS, Pinecone, Hugging Face, LangChain, LangSmith, FastAPI\\n* **Programming & Technical Skills:** Python, SQL, HTML, CSS, Scikit-learn, TensorFlow, Keras, Statistics\\n* **Data Science & Machine Learning:** Data Preprocessing, EDA, Feature Engineering, Model Training & Evaluation, Hyperparameter Tuning, Clustering, MLOps, Semantic Search, Retrieval-Augmented Generation (RAG), CNN, RNN, GPT, Transformers, Fine-Tuning, Prompt Engineering\\n* **Data Visualization & Analysis:** Pandas, NumPy, Matplotlib, Seaborn\\n\\n---\\n\\nhobbies section \\n\\n---\\n\\n### **Hobbies & Interests**\\nHobbies & Interests\\n\\n* Playing Cricket\\n* Watching Football\\n* Reading Books\\n* Exploring Latest Advancements in Artificial Intelligence\\n* Browsing the Internet for Tech & Knowledge Updates\\n\\n---\\n\\n### Contact Information\\n\\nContact Information'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': \"---\\n\\n### Contact Information\\n\\nContact Information\\n\\nüìû **Phone:** +91 93475 41040\\nüìß **Email:** [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\\nüíª **GitHub:** [github.com/ka1817](https://github.com/ka1817)\\nüîó **LinkedIn:** [linkedin.com/in/pranav-reddy-katta](https://www.linkedin.com/in/pranav-reddy-katta/)\\n---\\n\\n10th\\n---\\n\\n**Board of Secondary Education\\nTelangana State, India**\\n\\n**SECONDARY SCHOOL CERTIFICATE**\\n**REGULAR** PC/29/4222/04/256517/3\\n**TS-EE 524495**\\n\\n---\\n\\n**CERTIFIED THAT**\\n**KATTA SAI PRANAV REDDY**\\n**Father's Name:** KATTA SRINIVAS REDDY\\n**Mother's Name:** KATTA UMARANI\\n**Roll No.:** 1929100642\\n**Date of Birth:** 03/06/2003 (Zero Three June Two Zero Zero Three)\\n**School:** EKALAVYA FOUNDATION SCL NALGONDA, NALGONDA DISTRICT\\n**Medium:** ENGLISH\\n\\nHas appeared and **PASSED SSC EXAMINATION** held in **MARCH‚Äì2019**\\n\\n---\\n\\n### **The Candidate Secured the Following Grade and Grade Points in Curricular Areas:**\"}, page_content=\"---\\n\\n### Contact Information\\n\\nContact Information\\n\\nüìû **Phone:** +91 93475 41040\\nüìß **Email:** [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\\nüíª **GitHub:** [github.com/ka1817](https://github.com/ka1817)\\nüîó **LinkedIn:** [linkedin.com/in/pranav-reddy-katta](https://www.linkedin.com/in/pranav-reddy-katta/)\\n---\\n\\n10th\\n---\\n\\n**Board of Secondary Education\\nTelangana State, India**\\n\\n**SECONDARY SCHOOL CERTIFICATE**\\n**REGULAR** PC/29/4222/04/256517/3\\n**TS-EE 524495**\\n\\n---\\n\\n**CERTIFIED THAT**\\n**KATTA SAI PRANAV REDDY**\\n**Father's Name:** KATTA SRINIVAS REDDY\\n**Mother's Name:** KATTA UMARANI\\n**Roll No.:** 1929100642\\n**Date of Birth:** 03/06/2003 (Zero Three June Two Zero Zero Three)\\n**School:** EKALAVYA FOUNDATION SCL NALGONDA, NALGONDA DISTRICT\\n**Medium:** ENGLISH\\n\\nHas appeared and **PASSED SSC EXAMINATION** held in **MARCH‚Äì2019**\\n\\n---\\n\\n### **The Candidate Secured the Following Grade and Grade Points in Curricular Areas:**\"),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'Has appeared and **PASSED SSC EXAMINATION** held in **MARCH‚Äì2019**\\n\\n---\\n\\n### **The Candidate Secured the Following Grade and Grade Points in Curricular Areas:**\\n\\n| Subject                  | Grade FA | Grade SA | Overall Grade | Grade Point |\\n| ------------------------ | -------- | -------- | ------------- | ----------- |\\n| First Language (TELUGU)  | A1       | A1       | A1            | 10          |\\n| Third Language (ENGLISH) | A1       | A2       | A1            | 10          |\\n| Mathematics              | A1       | A1       | A1            | 10          |\\n| Science                  | A1       | A2       | A1            | 09          |\\n| Social Studies           | A1       | A1       | A1            | 10          |\\n| Second Language (HINDI)  | A1       | B1       | B1            | 08          |\\n\\n---\\n\\n### **Cumulative Grade Point Average (CGPA): 9.5**\\n\\n---\\n\\n### **CO-CURRICULAR AREAS:**'}, page_content='Has appeared and **PASSED SSC EXAMINATION** held in **MARCH‚Äì2019**\\n\\n---\\n\\n### **The Candidate Secured the Following Grade and Grade Points in Curricular Areas:**\\n\\n| Subject                  | Grade FA | Grade SA | Overall Grade | Grade Point |\\n| ------------------------ | -------- | -------- | ------------- | ----------- |\\n| First Language (TELUGU)  | A1       | A1       | A1            | 10          |\\n| Third Language (ENGLISH) | A1       | A2       | A1            | 10          |\\n| Mathematics              | A1       | A1       | A1            | 10          |\\n| Science                  | A1       | A2       | A1            | 09          |\\n| Social Studies           | A1       | A1       | A1            | 10          |\\n| Second Language (HINDI)  | A1       | B1       | B1            | 08          |\\n\\n---\\n\\n### **Cumulative Grade Point Average (CGPA): 9.5**\\n\\n---\\n\\n### **CO-CURRICULAR AREAS:**'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n### **Cumulative Grade Point Average (CGPA): 9.5**\\n\\n---\\n\\n### **CO-CURRICULAR AREAS:**\\n\\n| Subject                       | Grade |\\n| ----------------------------- | ----- |\\n| Value Education & Life Skills | A+    |\\n| Art & Cultural Education      | A+    |\\n| Work & Computer Education     | A+    |\\n| Physical & Health Education   | A+    |\\n\\n---\\n\\n### **Marks of Identification:**\\n\\n1. A MOLE ON THE LEFT HAND RING FINGER\\n2. A MOLE ON THE RIGHT ELBOW\\n\\n---\\n\\n**Head Master**\\n**EKALAVYA FOUNDATION SCHOOL**\\n**Nalgonda, Nalgonda**\\n**Date of Issue: 13th May, 2019**\\n\\n---\\n\\n**SECRETARY**\\n**Board of Secondary Education**\\n**Telangana State, Hyderabad**\\n\\n**Aadhaar No.:** 774291627518\\n\\n---\\n\\n12th(Intermediate)\\n---\\n\\n**Telangana State Board of Intermediate Education**\\nVidya Bhavan, Nampally, Hyderabad - 500 001'}, page_content='---\\n\\n### **Cumulative Grade Point Average (CGPA): 9.5**\\n\\n---\\n\\n### **CO-CURRICULAR AREAS:**\\n\\n| Subject                       | Grade |\\n| ----------------------------- | ----- |\\n| Value Education & Life Skills | A+    |\\n| Art & Cultural Education      | A+    |\\n| Work & Computer Education     | A+    |\\n| Physical & Health Education   | A+    |\\n\\n---\\n\\n### **Marks of Identification:**\\n\\n1. A MOLE ON THE LEFT HAND RING FINGER\\n2. A MOLE ON THE RIGHT ELBOW\\n\\n---\\n\\n**Head Master**\\n**EKALAVYA FOUNDATION SCHOOL**\\n**Nalgonda, Nalgonda**\\n**Date of Issue: 13th May, 2019**\\n\\n---\\n\\n**SECRETARY**\\n**Board of Secondary Education**\\n**Telangana State, Hyderabad**\\n\\n**Aadhaar No.:** 774291627518\\n\\n---\\n\\n12th(Intermediate)\\n---\\n\\n**Telangana State Board of Intermediate Education**\\nVidya Bhavan, Nampally, Hyderabad - 500 001'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '**Aadhaar No.:** 774291627518\\n\\n---\\n\\n12th(Intermediate)\\n---\\n\\n**Telangana State Board of Intermediate Education**\\nVidya Bhavan, Nampally, Hyderabad - 500 001\\n\\n**PASS CERTIFICATE-CUM-MEMORANDUM OF MARKS**\\n**This is to certify that**\\n**KATTA SAI PRANAV REDDY**\\n**Father‚Äôs Name:** KATTA SRINIVAS REDDY\\n**Mother‚Äôs Name:** KATTA UMARANI\\n**Registered Number:** 2158208799\\n**Month & Year of Exam:** MARCH 2021\\n**Medium:** ENGLISH\\n**Result:** A GRADE\\nhas appeared for the Intermediate Public Examination held in March 2021 and passed in the following subjects:\\n\\n---\\n\\n### **Part - I**\\n\\n**ENGLISH** ‚Äì 98 / 100\\n**SANSKRIT** ‚Äì 100 / 100\\n\\n---\\n\\n### **Part - II**\\n\\n**HE** ‚Äì 98 / 100\\n\\n---\\n\\n### **Part - III: Optional Subjects**'}, page_content='**Aadhaar No.:** 774291627518\\n\\n---\\n\\n12th(Intermediate)\\n---\\n\\n**Telangana State Board of Intermediate Education**\\nVidya Bhavan, Nampally, Hyderabad - 500 001\\n\\n**PASS CERTIFICATE-CUM-MEMORANDUM OF MARKS**\\n**This is to certify that**\\n**KATTA SAI PRANAV REDDY**\\n**Father‚Äôs Name:** KATTA SRINIVAS REDDY\\n**Mother‚Äôs Name:** KATTA UMARANI\\n**Registered Number:** 2158208799\\n**Month & Year of Exam:** MARCH 2021\\n**Medium:** ENGLISH\\n**Result:** A GRADE\\nhas appeared for the Intermediate Public Examination held in March 2021 and passed in the following subjects:\\n\\n---\\n\\n### **Part - I**\\n\\n**ENGLISH** ‚Äì 98 / 100\\n**SANSKRIT** ‚Äì 100 / 100\\n\\n---\\n\\n### **Part - II**\\n\\n**HE** ‚Äì 98 / 100\\n\\n---\\n\\n### **Part - III: Optional Subjects**'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n### **Part - I**\\n\\n**ENGLISH** ‚Äì 98 / 100\\n**SANSKRIT** ‚Äì 100 / 100\\n\\n---\\n\\n### **Part - II**\\n\\n**HE** ‚Äì 98 / 100\\n\\n---\\n\\n### **Part - III: Optional Subjects**\\n\\n| Subject              | Max Marks | Marks Obtained |\\n| -------------------- | --------- | -------------- |\\n| MATHEMATICS A        | 75        | 75             |\\n| MATHEMATICS B        | 75        | 75             |\\n| PHYSICS              | 60        | 58             |\\n| PHYSICS PRACTICALS   | 30        | 30             |\\n| CHEMISTRY            | 60        | 60             |\\n| CHEMISTRY PRACTICALS | 30        | 30             |\\n\\n---\\n\\n### **Environmental Education:** QUALIFIED\\n\\n### **Ethics and Human Values:** QUALIFIED\\n\\n---\\n\\n**Total Marks:**\\nIn Figures: **982**\\nIn Words: **NINE EIGHT TWO**\\n\\n---\\n\\n**Date:** 28-06-2021\\n**Signature of the Principal and College Seal:** Sri Chaitanya Jr. Kalasala\\n**Signature:** (Controller of Examinations)\\n\\n---'}, page_content='---\\n\\n### **Part - I**\\n\\n**ENGLISH** ‚Äì 98 / 100\\n**SANSKRIT** ‚Äì 100 / 100\\n\\n---\\n\\n### **Part - II**\\n\\n**HE** ‚Äì 98 / 100\\n\\n---\\n\\n### **Part - III: Optional Subjects**\\n\\n| Subject              | Max Marks | Marks Obtained |\\n| -------------------- | --------- | -------------- |\\n| MATHEMATICS A        | 75        | 75             |\\n| MATHEMATICS B        | 75        | 75             |\\n| PHYSICS              | 60        | 58             |\\n| PHYSICS PRACTICALS   | 30        | 30             |\\n| CHEMISTRY            | 60        | 60             |\\n| CHEMISTRY PRACTICALS | 30        | 30             |\\n\\n---\\n\\n### **Environmental Education:** QUALIFIED\\n\\n### **Ethics and Human Values:** QUALIFIED\\n\\n---\\n\\n**Total Marks:**\\nIn Figures: **982**\\nIn Words: **NINE EIGHT TWO**\\n\\n---\\n\\n**Date:** 28-06-2021\\n**Signature of the Principal and College Seal:** Sri Chaitanya Jr. Kalasala\\n**Signature:** (Controller of Examinations)\\n\\n---'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n**Date:** 28-06-2021\\n**Signature of the Principal and College Seal:** Sri Chaitanya Jr. Kalasala\\n**Signature:** (Controller of Examinations)\\n\\n---\\n\\nResume \\n---\\n**Katta Sai Pranav Reddy**\\nEmail: [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\\nGitHub: ka1817\\nLinkedIn: pranav-reddy-katta\\n\\n---\\n\\n### **Professional Summary**\\n\\nAI and ML Engineer skilled in developing end-to-end machine learning and Generative AI solutions for real-world business challenges. Proficient in data preprocessing, exploratory data analysis, and building predictive models to deliver actionable insights. Experienced in leveraging advanced AI techniques and data-driven strategies to create scalable, impactful solutions.\\n\\n---\\n\\n# Education\\n\\n* **Anurag University**, Hyderabad, India\\n  B.Tech in Artificial Intelligence and Machine Learning; CGPA: 8.29\\n  *09/2021 ‚Äì 04/2025*\\n\\n* **Sri Chaitanya Junior College**, Hyderabad, India\\n  MPC (Maths, Physics, Chemistry); Percentage: 98%\\n  *06/2019 ‚Äì 05/2021*'}, page_content='---\\n\\n**Date:** 28-06-2021\\n**Signature of the Principal and College Seal:** Sri Chaitanya Jr. Kalasala\\n**Signature:** (Controller of Examinations)\\n\\n---\\n\\nResume \\n---\\n**Katta Sai Pranav Reddy**\\nEmail: [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\\nGitHub: ka1817\\nLinkedIn: pranav-reddy-katta\\n\\n---\\n\\n### **Professional Summary**\\n\\nAI and ML Engineer skilled in developing end-to-end machine learning and Generative AI solutions for real-world business challenges. Proficient in data preprocessing, exploratory data analysis, and building predictive models to deliver actionable insights. Experienced in leveraging advanced AI techniques and data-driven strategies to create scalable, impactful solutions.\\n\\n---\\n\\n# Education\\n\\n* **Anurag University**, Hyderabad, India\\n  B.Tech in Artificial Intelligence and Machine Learning; CGPA: 8.29\\n  *09/2021 ‚Äì 04/2025*\\n\\n* **Sri Chaitanya Junior College**, Hyderabad, India\\n  MPC (Maths, Physics, Chemistry); Percentage: 98%\\n  *06/2019 ‚Äì 05/2021*'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '* **Sri Chaitanya Junior College**, Hyderabad, India\\n  MPC (Maths, Physics, Chemistry); Percentage: 98%\\n  *06/2019 ‚Äì 05/2021*\\n\\n-----------------------------------------\\n\\n### Pranav Work Experience\\n\\nWork Experience\\n\\niNeuron Intelligence Pvt. Ltd.(Remote)\\nMachine Learning Intern ‚Äî 10/2024 ‚Äì 11/2024'}, page_content='* **Sri Chaitanya Junior College**, Hyderabad, India\\n  MPC (Maths, Physics, Chemistry); Percentage: 98%\\n  *06/2019 ‚Äì 05/2021*\\n\\n-----------------------------------------\\n\\n### Pranav Work Experience\\n\\nWork Experience\\n\\niNeuron Intelligence Pvt. Ltd.(Remote)\\nMachine Learning Intern ‚Äî 10/2024 ‚Äì 11/2024'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '-----------------------------------------\\n\\n### Pranav Work Experience\\n\\nWork Experience\\n\\niNeuron Intelligence Pvt. Ltd.(Remote)\\nMachine Learning Intern ‚Äî 10/2024 ‚Äì 11/2024\\n\\n1.Conducted extensive data preprocessing and exploratory data analysis (EDA) on large customer datasets to identify key behavioral patterns and high-value customer segments.\\n2.Developed and trained machine learning models for customer segmentation using clustering techniques such as K-Means and Hierarchical Clustering, enhancing marketing strategy alignment.\\n3.Collaborated with cross-functional teams to interpret analytical insights and monitored model performance across different stages of the pipeline, ensuring accuracy and consistency.\\n4.Delivered actionable recommendations based on statistical analysis and predictive modeling, supporting data-driven decision-making for targeted marketing campaigns.'}, page_content='-----------------------------------------\\n\\n### Pranav Work Experience\\n\\nWork Experience\\n\\niNeuron Intelligence Pvt. Ltd.(Remote)\\nMachine Learning Intern ‚Äî 10/2024 ‚Äì 11/2024\\n\\n1.Conducted extensive data preprocessing and exploratory data analysis (EDA) on large customer datasets to identify key behavioral patterns and high-value customer segments.\\n2.Developed and trained machine learning models for customer segmentation using clustering techniques such as K-Means and Hierarchical Clustering, enhancing marketing strategy alignment.\\n3.Collaborated with cross-functional teams to interpret analytical insights and monitored model performance across different stages of the pipeline, ensuring accuracy and consistency.\\n4.Delivered actionable recommendations based on statistical analysis and predictive modeling, supporting data-driven decision-making for targeted marketing campaigns.'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': \"Unified Mentor Pvt. Ltd.(Remote)\\nData Science Intern ‚Äî 09/2024 ‚Äì 10/2024\\n1. Developed and optimized machine learning models to predict employee attrition, enabling proactive retention strategies and improving workforce stability.\\n2. Conducted comprehensive data preprocessing, feature engineering, and exploratory data analysis (EDA) to identify key factors influencing employee turnover.\\n3. Delivered actionable insights and visualized patterns through dashboards and reports, supporting HR teams in making data-driven decisions.\\n4. Presented findings to stakeholders, translating complex analytics into clear, strategic recommendations for reducing attrition risk.\\n\\n\\n-------------------------\\n\\n# Projects\\n\\nPranav Reddy's Projects\\n\\n1. üõí BigBasket SmartCart ‚Äì AI Assistant for BigBasket Shopping\\n---\\n## üßæ Introduction\"}, page_content=\"Unified Mentor Pvt. Ltd.(Remote)\\nData Science Intern ‚Äî 09/2024 ‚Äì 10/2024\\n1. Developed and optimized machine learning models to predict employee attrition, enabling proactive retention strategies and improving workforce stability.\\n2. Conducted comprehensive data preprocessing, feature engineering, and exploratory data analysis (EDA) to identify key factors influencing employee turnover.\\n3. Delivered actionable insights and visualized patterns through dashboards and reports, supporting HR teams in making data-driven decisions.\\n4. Presented findings to stakeholders, translating complex analytics into clear, strategic recommendations for reducing attrition risk.\\n\\n\\n-------------------------\\n\\n# Projects\\n\\nPranav Reddy's Projects\\n\\n1. üõí BigBasket SmartCart ‚Äì AI Assistant for BigBasket Shopping\\n---\\n## üßæ Introduction\"),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': \"-------------------------\\n\\n# Projects\\n\\nPranav Reddy's Projects\\n\\n1. üõí BigBasket SmartCart ‚Äì AI Assistant for BigBasket Shopping\\n---\\n## üßæ Introduction\\n\\nThe rapid evolution of AI technologies has created new opportunities for enhancing user experience in digital commerce. Leveraging state-of-the-art language models and retrieval systems, intelligent assistants can now understand complex queries, process vast amounts of product data, and deliver precise, context-aware responses. This project presents a scalable and robust AI-powered shopping assistant tailored for BigBasket's product ecosystem. Built using Retrieval-Augmented Generation (RAG), vector embeddings, and large language models (LLMs), the system enables efficient and intelligent product discovery through natural language interaction.\\n\\n---\\n\\n## ‚ùó Problem Statement\"}, page_content=\"-------------------------\\n\\n# Projects\\n\\nPranav Reddy's Projects\\n\\n1. üõí BigBasket SmartCart ‚Äì AI Assistant for BigBasket Shopping\\n---\\n## üßæ Introduction\\n\\nThe rapid evolution of AI technologies has created new opportunities for enhancing user experience in digital commerce. Leveraging state-of-the-art language models and retrieval systems, intelligent assistants can now understand complex queries, process vast amounts of product data, and deliver precise, context-aware responses. This project presents a scalable and robust AI-powered shopping assistant tailored for BigBasket's product ecosystem. Built using Retrieval-Augmented Generation (RAG), vector embeddings, and large language models (LLMs), the system enables efficient and intelligent product discovery through natural language interaction.\\n\\n---\\n\\n## ‚ùó Problem Statement\"),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n## ‚ùó Problem Statement\\n\\nOnline shoppers frequently seek personalized and context-specific product recommendations, such as identifying the best-rated skincare item at the lowest price. However, conventional search systems often fall short in understanding such nuanced queries, lacking the ability to interpret intent, compare attributes across products, and deliver concise, relevant results. This creates friction in the user journey, leading to suboptimal shopping experiences. There is a clear need for an intelligent assistant that can process natural language queries, reason over structured product data, and deliver accurate, insightful responses to aid decision-making.\\n\\n---\\n\\n## Business Goal:\\n\\nTo enhance the shopping experience, boost conversion rates, and optimize search efficiency by enabling natural language-based product search that understands user intent and delivers context-aware, personalized recommendations.\\n\\n\\n\\n## üí∞ Business Impact (Revenue + Cost)'}, page_content='---\\n\\n## ‚ùó Problem Statement\\n\\nOnline shoppers frequently seek personalized and context-specific product recommendations, such as identifying the best-rated skincare item at the lowest price. However, conventional search systems often fall short in understanding such nuanced queries, lacking the ability to interpret intent, compare attributes across products, and deliver concise, relevant results. This creates friction in the user journey, leading to suboptimal shopping experiences. There is a clear need for an intelligent assistant that can process natural language queries, reason over structured product data, and deliver accurate, insightful responses to aid decision-making.\\n\\n---\\n\\n## Business Goal:\\n\\nTo enhance the shopping experience, boost conversion rates, and optimize search efficiency by enabling natural language-based product search that understands user intent and delivers context-aware, personalized recommendations.\\n\\n\\n\\n## üí∞ Business Impact (Revenue + Cost)'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '## üí∞ Business Impact (Revenue + Cost)\\n\\nüí∏ 1. Increased Conversion Rates (‚Üë Revenue)\\n\\n    ‚Ä¢ Users find relevant products faster, leading to more product views, cart adds, and purchases\\n\\n    ‚Ä¢ Personalized recommendations match buyer intent better than traditional search\\n\\n    ‚Ä¢ Better UX = lower drop-off rates\\n\\nüìà Even a 1‚Äì2% uplift in conversions from improved product search can lead to significant revenue gains for a large marketplace like BigBasket.\\n\\nüìâ 2. Reduced Customer Support Queries (‚Üì Cost)\\n\\n    ‚Ä¢ AI assistant can handle informational and product-related queries\\n\\n    ‚Ä¢ Reduces manual intervention, live chat support, and email volume\\n\\n    ‚Ä¢ More self-service = less operational overhead\\n\\n‚è±Ô∏è 3. Reduced Time-to-Purchase (‚Üë Efficiency)\\n\\n    ‚Ä¢ Customers make faster decisions because the assistant summarizes comparisons (e.g., price vs. rating trade-offs)\\n\\n    ‚Ä¢ This shortens the purchase journey and increases user satisfaction\\n\\nüß™ 4. Rapid Experimentation & Deployment (‚Üì Dev Costs)'}, page_content='## üí∞ Business Impact (Revenue + Cost)\\n\\nüí∏ 1. Increased Conversion Rates (‚Üë Revenue)\\n\\n    ‚Ä¢ Users find relevant products faster, leading to more product views, cart adds, and purchases\\n\\n    ‚Ä¢ Personalized recommendations match buyer intent better than traditional search\\n\\n    ‚Ä¢ Better UX = lower drop-off rates\\n\\nüìà Even a 1‚Äì2% uplift in conversions from improved product search can lead to significant revenue gains for a large marketplace like BigBasket.\\n\\nüìâ 2. Reduced Customer Support Queries (‚Üì Cost)\\n\\n    ‚Ä¢ AI assistant can handle informational and product-related queries\\n\\n    ‚Ä¢ Reduces manual intervention, live chat support, and email volume\\n\\n    ‚Ä¢ More self-service = less operational overhead\\n\\n‚è±Ô∏è 3. Reduced Time-to-Purchase (‚Üë Efficiency)\\n\\n    ‚Ä¢ Customers make faster decisions because the assistant summarizes comparisons (e.g., price vs. rating trade-offs)\\n\\n    ‚Ä¢ This shortens the purchase journey and increases user satisfaction\\n\\nüß™ 4. Rapid Experimentation & Deployment (‚Üì Dev Costs)'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '‚Ä¢ This shortens the purchase journey and increases user satisfaction\\n\\nüß™ 4. Rapid Experimentation & Deployment (‚Üì Dev Costs)\\n\\n    ‚Ä¢ The project is modular, Dockerized, and CI/CD enabled ‚Üí easier to iterate and deploy\\n\\n    ‚Ä¢ Can be extended to other verticals (electronics, fashion) or other marketplaces with minimal changes\\n\\n---\\n\\n## üöÄ Features\\n\\nüîç Natural Language Product Search\\nUsers can ask queries like \"cheapest skin care with highest rating\" or \"best perfume under ‚Çπ500\".\\n\\nüß† Query Rewriting with LLM\\nUses Groq LLMs (gemma2-9b-it) to refine user queries for more precise retrieval.\\n\\nüìÑ Document Embedding & Vector Search\\nPreprocessed BigBasket product data embedded with thenlper/gte-small and indexed using FAISS.\\n\\nü§ñ RAG Pipeline\\nUses llama3-70b-8192 model for final answer generation based on retrieved and reranked results.\\n\\nüîÅ Reranking with CrossEncoder\\nImproves accuracy using cross-encoder/ms-marco-MiniLM-L-6-v2.'}, page_content='‚Ä¢ This shortens the purchase journey and increases user satisfaction\\n\\nüß™ 4. Rapid Experimentation & Deployment (‚Üì Dev Costs)\\n\\n    ‚Ä¢ The project is modular, Dockerized, and CI/CD enabled ‚Üí easier to iterate and deploy\\n\\n    ‚Ä¢ Can be extended to other verticals (electronics, fashion) or other marketplaces with minimal changes\\n\\n---\\n\\n## üöÄ Features\\n\\nüîç Natural Language Product Search\\nUsers can ask queries like \"cheapest skin care with highest rating\" or \"best perfume under ‚Çπ500\".\\n\\nüß† Query Rewriting with LLM\\nUses Groq LLMs (gemma2-9b-it) to refine user queries for more precise retrieval.\\n\\nüìÑ Document Embedding & Vector Search\\nPreprocessed BigBasket product data embedded with thenlper/gte-small and indexed using FAISS.\\n\\nü§ñ RAG Pipeline\\nUses llama3-70b-8192 model for final answer generation based on retrieved and reranked results.\\n\\nüîÅ Reranking with CrossEncoder\\nImproves accuracy using cross-encoder/ms-marco-MiniLM-L-6-v2.'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üîÅ Reranking with CrossEncoder\\nImproves accuracy using cross-encoder/ms-marco-MiniLM-L-6-v2.\\n\\nüåê FastAPI Backend\\nEasily accessible via localhost:8000 or deployed server.\\n\\nüê≥ Dockerized\\nBuild once, run anywhere. Fully containerized using Docker.\\n\\nüö∞ CI/CD with GitHub Actions\\nAutomated testing, image build, and push to DockerHub.\\n\\nüìú Logging\\nLogging implemented for each step in the pipeline for transparency and debugging.\\n\\n---\\n\\n## üóÇÔ∏è Folder Structure'}, page_content='üîÅ Reranking with CrossEncoder\\nImproves accuracy using cross-encoder/ms-marco-MiniLM-L-6-v2.\\n\\nüåê FastAPI Backend\\nEasily accessible via localhost:8000 or deployed server.\\n\\nüê≥ Dockerized\\nBuild once, run anywhere. Fully containerized using Docker.\\n\\nüö∞ CI/CD with GitHub Actions\\nAutomated testing, image build, and push to DockerHub.\\n\\nüìú Logging\\nLogging implemented for each step in the pipeline for transparency and debugging.\\n\\n---\\n\\n## üóÇÔ∏è Folder Structure'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üìú Logging\\nLogging implemented for each step in the pipeline for transparency and debugging.\\n\\n---\\n\\n## üóÇÔ∏è Folder Structure\\n\\n```bash\\nBIGBASKET/\\n‚îú‚îÄ‚îÄ .github/\\n‚îÇ   ‚îî‚îÄ‚îÄ workflows/\\n‚îÇ       ‚îî‚îÄ‚îÄ ci-cd.yml\\n‚îú‚îÄ‚îÄ data/\\n‚îÇ   ‚îî‚îÄ‚îÄ BigBasket Products.csv\\n‚îú‚îÄ‚îÄ logs/\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.log\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.log\\n‚îÇ   ‚îú‚îÄ‚îÄ query_rewriting.log\\n‚îÇ   ‚îî‚îÄ‚îÄ retrieval_generation.log\\n‚îú‚îÄ‚îÄ src/\\n‚îÇ   ‚îú‚îÄ‚îÄ utils/\\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py\\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py\\n‚îÇ   ‚îú‚îÄ‚îÄ query_rewritting.py\\n‚îÇ   ‚îî‚îÄ‚îÄ retrival_genaration.py\\n‚îú‚îÄ‚îÄ static/\\n‚îÇ   ‚îî‚îÄ‚îÄ css/\\n‚îÇ       ‚îî‚îÄ‚îÄ style.css\\n‚îú‚îÄ‚îÄ templates/\\n‚îÇ   ‚îî‚îÄ‚îÄ index.html\\n‚îú‚îÄ‚îÄ tests/\\n‚îú‚îÄ‚îÄ ui/\\n‚îú‚îÄ‚îÄ main.py\\n‚îú‚îÄ‚îÄ Dockerfile\\n‚îú‚îÄ‚îÄ requirements.txt\\n‚îú‚îÄ‚îÄ .env\\n‚îú‚îÄ‚îÄ .dockerignore\\n‚îú‚îÄ‚îÄ .gitignore\\n‚îî‚îÄ‚îÄ README.md\\n```\\n\\n---\\n\\n# üß™ Local Development Setup\\n\\n```bash\\n# Clone the repository\\ngit clone https://github.com/ka1817/BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\ncd BigBasket'}, page_content='üìú Logging\\nLogging implemented for each step in the pipeline for transparency and debugging.\\n\\n---\\n\\n## üóÇÔ∏è Folder Structure\\n\\n```bash\\nBIGBASKET/\\n‚îú‚îÄ‚îÄ .github/\\n‚îÇ   ‚îî‚îÄ‚îÄ workflows/\\n‚îÇ       ‚îî‚îÄ‚îÄ ci-cd.yml\\n‚îú‚îÄ‚îÄ data/\\n‚îÇ   ‚îî‚îÄ‚îÄ BigBasket Products.csv\\n‚îú‚îÄ‚îÄ logs/\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.log\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.log\\n‚îÇ   ‚îú‚îÄ‚îÄ query_rewriting.log\\n‚îÇ   ‚îî‚îÄ‚îÄ retrieval_generation.log\\n‚îú‚îÄ‚îÄ src/\\n‚îÇ   ‚îú‚îÄ‚îÄ utils/\\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py\\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py\\n‚îÇ   ‚îú‚îÄ‚îÄ query_rewritting.py\\n‚îÇ   ‚îî‚îÄ‚îÄ retrival_genaration.py\\n‚îú‚îÄ‚îÄ static/\\n‚îÇ   ‚îî‚îÄ‚îÄ css/\\n‚îÇ       ‚îî‚îÄ‚îÄ style.css\\n‚îú‚îÄ‚îÄ templates/\\n‚îÇ   ‚îî‚îÄ‚îÄ index.html\\n‚îú‚îÄ‚îÄ tests/\\n‚îú‚îÄ‚îÄ ui/\\n‚îú‚îÄ‚îÄ main.py\\n‚îú‚îÄ‚îÄ Dockerfile\\n‚îú‚îÄ‚îÄ requirements.txt\\n‚îú‚îÄ‚îÄ .env\\n‚îú‚îÄ‚îÄ .dockerignore\\n‚îú‚îÄ‚îÄ .gitignore\\n‚îî‚îÄ‚îÄ README.md\\n```\\n\\n---\\n\\n# üß™ Local Development Setup\\n\\n```bash\\n# Clone the repository\\ngit clone https://github.com/ka1817/BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\ncd BigBasket'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n# üß™ Local Development Setup\\n\\n```bash\\n# Clone the repository\\ngit clone https://github.com/ka1817/BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\ncd BigBasket\\n\\n# Create virtual environment\\npython -m venv venv\\nsource venv/bin/activate   # On Windows: venv\\\\Scripts\\\\activate\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Before Running the app set .env(environment variable GROQ_API_KEY)\\nuvicorn main:app --reload --port 8000\\n```\\n\\n## üê≥ Docker Instructions\\n\\nüîß 1. Pull Image\\n\\n```bash\\ndocker pull pranavreddy123/bigbasket-assistant:latest\\n```\\n\\nüöÄ 2. Run the App (Detached Mode)\\n\\n```bash\\ndocker run -d -p 8000:8000 \\\\\\n-e GROQ_API_KEY=create groq api from groq cloud \\\\\\npranavreddy123/bigbasket-assistant:latest\\n```\\n\\nüåê 3. Access the App\\n\\n```bash\\nhttp://localhost:8000\\n```\\n\\n---\\n\\nü§ñ Example Usage'}, page_content='---\\n\\n# üß™ Local Development Setup\\n\\n```bash\\n# Clone the repository\\ngit clone https://github.com/ka1817/BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\ncd BigBasket\\n\\n# Create virtual environment\\npython -m venv venv\\nsource venv/bin/activate   # On Windows: venv\\\\Scripts\\\\activate\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Before Running the app set .env(environment variable GROQ_API_KEY)\\nuvicorn main:app --reload --port 8000\\n```\\n\\n## üê≥ Docker Instructions\\n\\nüîß 1. Pull Image\\n\\n```bash\\ndocker pull pranavreddy123/bigbasket-assistant:latest\\n```\\n\\nüöÄ 2. Run the App (Detached Mode)\\n\\n```bash\\ndocker run -d -p 8000:8000 \\\\\\n-e GROQ_API_KEY=create groq api from groq cloud \\\\\\npranavreddy123/bigbasket-assistant:latest\\n```\\n\\nüåê 3. Access the App\\n\\n```bash\\nhttp://localhost:8000\\n```\\n\\n---\\n\\nü§ñ Example Usage'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üåê 3. Access the App\\n\\n```bash\\nhttp://localhost:8000\\n```\\n\\n---\\n\\nü§ñ Example Usage\\n\\nQuery: \"Which is the cheapest hair product with high rating?\"\\nRewritten: \"Find the most affordable hair care product with a high customer rating.\"\\nResponse: \"Garlic Oil - Vegetarian Capsule 500 mg by Sri Sri Ayurveda is available at ‚Çπ220 with a 4.1 rating.\"\\n\\n---\\n\\n## üõ†Ô∏è GitHub Actions (CI/CD)\\n\\nFile: .github/workflows/ci-cd.yml\\n\\n‚úÖ CI-Test: Runs unit tests using pytest.\\n\\nüê≥ CD-Docker: Builds Docker image and pushes to DockerHub.\\n\\nTriggered on push to main or pull request.\\n\\n---\\n\\n## ‚òÅÔ∏è Deployment on Amazon EC2\\n\\n### 1. Launch EC2 Instance (Ubuntu 20.04)\\n\\n### 2. SSH into your instance\\n\\n```bash\\nssh -i \"your-key.pem\" ubuntu@your-ec2-ip\\n```\\n\\n### 3. Install Docker\\n\\n```bash\\nsudo apt update\\nsudo apt install docker.io -y\\nsudo systemctl start docker\\nsudo systemctl enable docker\\n```\\n\\n### 4. Pull and Run Docker Image'}, page_content='üåê 3. Access the App\\n\\n```bash\\nhttp://localhost:8000\\n```\\n\\n---\\n\\nü§ñ Example Usage\\n\\nQuery: \"Which is the cheapest hair product with high rating?\"\\nRewritten: \"Find the most affordable hair care product with a high customer rating.\"\\nResponse: \"Garlic Oil - Vegetarian Capsule 500 mg by Sri Sri Ayurveda is available at ‚Çπ220 with a 4.1 rating.\"\\n\\n---\\n\\n## üõ†Ô∏è GitHub Actions (CI/CD)\\n\\nFile: .github/workflows/ci-cd.yml\\n\\n‚úÖ CI-Test: Runs unit tests using pytest.\\n\\nüê≥ CD-Docker: Builds Docker image and pushes to DockerHub.\\n\\nTriggered on push to main or pull request.\\n\\n---\\n\\n## ‚òÅÔ∏è Deployment on Amazon EC2\\n\\n### 1. Launch EC2 Instance (Ubuntu 20.04)\\n\\n### 2. SSH into your instance\\n\\n```bash\\nssh -i \"your-key.pem\" ubuntu@your-ec2-ip\\n```\\n\\n### 3. Install Docker\\n\\n```bash\\nsudo apt update\\nsudo apt install docker.io -y\\nsudo systemctl start docker\\nsudo systemctl enable docker\\n```\\n\\n### 4. Pull and Run Docker Image'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '### 3. Install Docker\\n\\n```bash\\nsudo apt update\\nsudo apt install docker.io -y\\nsudo systemctl start docker\\nsudo systemctl enable docker\\n```\\n\\n### 4. Pull and Run Docker Image\\n\\n```bash\\ndocker pull pranavreddy123/bigbasket-assistant:latest\\n# Ensure your .env file is in the same directory, or create an API key using Groq Cloud and add it to the .env file\\ndocker run -d --env-file .env -p 8000:8000 pranavreddy123/bigbasket-assistant:latest\\n```\\n\\n## Access your app via `http://<your-ec2-public-ip>`\\n\\n## üß† Tech Stack\\n\\n‚úÖ LLMs: Groq (gemma2-9b-it, llama3-70b-8192)\\n\\n‚úÖ LangChain, FAISS, HuggingFace, CrossEncoder\\n\\n‚úÖ FastAPI\\n\\n‚úÖ Docker\\n\\n‚úÖ GitHub Actions\\n\\n‚úÖ AWS EC2\\n\\n‚úÖ HTML/CSS\\n\\n---\\n\\n## üîó Links\\n\\nüîç GitHub Repo: BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\n\\nüê≥ DockerHub: pranavreddy123/bigbasket-assistant\\n\\n---\\n\\n## üßë\\u200düíª Developed By\\n\\nPranav Reddy\\n\\n\\n2.Netflix Customer Churn Prediction ‚Äì End-to-End ML System** *(Personal Project)* \\\\[GitHub]'}, page_content='### 3. Install Docker\\n\\n```bash\\nsudo apt update\\nsudo apt install docker.io -y\\nsudo systemctl start docker\\nsudo systemctl enable docker\\n```\\n\\n### 4. Pull and Run Docker Image\\n\\n```bash\\ndocker pull pranavreddy123/bigbasket-assistant:latest\\n# Ensure your .env file is in the same directory, or create an API key using Groq Cloud and add it to the .env file\\ndocker run -d --env-file .env -p 8000:8000 pranavreddy123/bigbasket-assistant:latest\\n```\\n\\n## Access your app via `http://<your-ec2-public-ip>`\\n\\n## üß† Tech Stack\\n\\n‚úÖ LLMs: Groq (gemma2-9b-it, llama3-70b-8192)\\n\\n‚úÖ LangChain, FAISS, HuggingFace, CrossEncoder\\n\\n‚úÖ FastAPI\\n\\n‚úÖ Docker\\n\\n‚úÖ GitHub Actions\\n\\n‚úÖ AWS EC2\\n\\n‚úÖ HTML/CSS\\n\\n---\\n\\n## üîó Links\\n\\nüîç GitHub Repo: BigBasket-SmartCart-AI-Assistant-for-BigBasket-Shopping\\n\\nüê≥ DockerHub: pranavreddy123/bigbasket-assistant\\n\\n---\\n\\n## üßë\\u200düíª Developed By\\n\\nPranav Reddy\\n\\n\\n2.Netflix Customer Churn Prediction ‚Äì End-to-End ML System** *(Personal Project)* \\\\[GitHub]'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üê≥ DockerHub: pranavreddy123/bigbasket-assistant\\n\\n---\\n\\n## üßë\\u200düíª Developed By\\n\\nPranav Reddy\\n\\n\\n2.Netflix Customer Churn Prediction ‚Äì End-to-End ML System** *(Personal Project)* \\\\[GitHub]\\n\\n* Developed a complete machine learning pipeline to predict customer churn, achieving 99% recall and 0.99 ROC AUC through feature engineering, hyperparameter tuning, and cross-validation.\\n* Performed in-depth EDA to identify key churn drivers such as low engagement, infrequent logins, and payment methods, improving model interpretability and business insights.\\n* Implemented reproducible MLOps workflows with data versioning using DVC and AWS S3, and tracked experiments, metrics, and model artifacts using MLflow.\\n* Designed and deployed a FastAPI-based REST API with HTML/CSS frontend for real-time predictions, containerized the application using Docker, and automated CI/CD using GitHub Actions for deployment on AWS EC2.\\n\\nüìä Netflix Customer Churn Prediction\\n# üìä Netflix Customer Churn Prediction'}, page_content='üê≥ DockerHub: pranavreddy123/bigbasket-assistant\\n\\n---\\n\\n## üßë\\u200düíª Developed By\\n\\nPranav Reddy\\n\\n\\n2.Netflix Customer Churn Prediction ‚Äì End-to-End ML System** *(Personal Project)* \\\\[GitHub]\\n\\n* Developed a complete machine learning pipeline to predict customer churn, achieving 99% recall and 0.99 ROC AUC through feature engineering, hyperparameter tuning, and cross-validation.\\n* Performed in-depth EDA to identify key churn drivers such as low engagement, infrequent logins, and payment methods, improving model interpretability and business insights.\\n* Implemented reproducible MLOps workflows with data versioning using DVC and AWS S3, and tracked experiments, metrics, and model artifacts using MLflow.\\n* Designed and deployed a FastAPI-based REST API with HTML/CSS frontend for real-time predictions, containerized the application using Docker, and automated CI/CD using GitHub Actions for deployment on AWS EC2.\\n\\nüìä Netflix Customer Churn Prediction\\n# üìä Netflix Customer Churn Prediction'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': 'üìä Netflix Customer Churn Prediction\\n# üìä Netflix Customer Churn Prediction\\n\\n#  Project Overview\\n\\nNetflix, like many subscription-based platforms, faces the challenge of customer churn. Retaining existing customers is significantly more cost-effective than acquiring new ones. This project delivers a full-scale machine learning solution to predict customer churn using behavioral and subscription data, from ingestion to deployment via a FastAPI interface.\\n\\nThis repository presents a production-grade, explainable, and reproducible ML pipeline with CI/CD, experiment tracking (**MLflow**), data versioning (**DVC**), and containerized deployment using **Docker**.\\n\\n---\\n\\n#  Problem Statement\\n\\nNetflix seeks to proactively identify users likely to cancel their subscriptions. Predicting churn enables targeted interventions to retain users and minimize revenue loss.\\n\\n> **Goal:** Build an ML classification model that predicts churn based on customer behavior and plan details.\\n\\n---'}, page_content='üìä Netflix Customer Churn Prediction\\n# üìä Netflix Customer Churn Prediction\\n\\n#  Project Overview\\n\\nNetflix, like many subscription-based platforms, faces the challenge of customer churn. Retaining existing customers is significantly more cost-effective than acquiring new ones. This project delivers a full-scale machine learning solution to predict customer churn using behavioral and subscription data, from ingestion to deployment via a FastAPI interface.\\n\\nThis repository presents a production-grade, explainable, and reproducible ML pipeline with CI/CD, experiment tracking (**MLflow**), data versioning (**DVC**), and containerized deployment using **Docker**.\\n\\n---\\n\\n#  Problem Statement\\n\\nNetflix seeks to proactively identify users likely to cancel their subscriptions. Predicting churn enables targeted interventions to retain users and minimize revenue loss.\\n\\n> **Goal:** Build an ML classification model that predicts churn based on customer behavior and plan details.\\n\\n---'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '> **Goal:** Build an ML classification model that predicts churn based on customer behavior and plan details.\\n\\n---\\n\\n##  Key Features Used\\n\\n| Feature                    | Type        | Description                                    |\\n| -------------------------- | ----------- | ---------------------------------------------- |\\n| watch\\\\_hours               | Numerical   | Total hours watched                            |\\n| last\\\\_login\\\\_days          | Numerical   | Days since last login                          |\\n| number\\\\_of\\\\_profiles       | Numerical   | Total profiles under the account               |\\n| avg\\\\_watch\\\\_time\\\\_per\\\\_day | Numerical   | Daily average watch time                       |\\n| subscription\\\\_type         | Categorical | Subscription level: Basic, Standard, Premium   |\\n| payment\\\\_method            | Categorical | Payment method: Credit Card, UPI, PayPal, etc. |\\n| churned                    | Target      | 1 = Churned, 0 = Not churned                   |\\n\\n---'}, page_content='> **Goal:** Build an ML classification model that predicts churn based on customer behavior and plan details.\\n\\n---\\n\\n##  Key Features Used\\n\\n| Feature                    | Type        | Description                                    |\\n| -------------------------- | ----------- | ---------------------------------------------- |\\n| watch\\\\_hours               | Numerical   | Total hours watched                            |\\n| last\\\\_login\\\\_days          | Numerical   | Days since last login                          |\\n| number\\\\_of\\\\_profiles       | Numerical   | Total profiles under the account               |\\n| avg\\\\_watch\\\\_time\\\\_per\\\\_day | Numerical   | Daily average watch time                       |\\n| subscription\\\\_type         | Categorical | Subscription level: Basic, Standard, Premium   |\\n| payment\\\\_method            | Categorical | Payment method: Credit Card, UPI, PayPal, etc. |\\n| churned                    | Target      | 1 = Churned, 0 = Not churned                   |\\n\\n---'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n##  Key EDA Insights\\n\\n### üî¨ Feature Significance\\n\\n| Feature                    | Test           | p-value | Significant? |\\n| -------------------------- | -------------- | ------- | ------------ |\\n| subscription\\\\_type         | Chi-Square     | 0.0000  | ‚úÖ Yes        |\\n| payment\\\\_method            | Chi-Square     | 0.0000  | ‚úÖ Yes        |\\n| number\\\\_of\\\\_profiles       | Chi-Square     | 0.0000  | ‚úÖ Yes        |\\n| watch\\\\_hours               | Mann-Whitney U | 0.0000  | ‚úÖ Yes        |\\n| last\\\\_login\\\\_days          | Mann-Whitney U | 0.0000  | ‚úÖ Yes        |\\n| avg\\\\_watch\\\\_time\\\\_per\\\\_day | Mann-Whitney U | 0.0000  | ‚úÖ Yes        |\\n| age                        | Mann-Whitney U | 0.7803  | ‚ùå No         |\\n| gender, region, device     | Chi-Square     | > 0.3   | ‚ùå No         |\\n\\n> ‚úÖ These statistically significant features were included in the final model pipeline.\\n\\n---\\n\\n##  Project Architecture'}, page_content='---\\n\\n##  Key EDA Insights\\n\\n### üî¨ Feature Significance\\n\\n| Feature                    | Test           | p-value | Significant? |\\n| -------------------------- | -------------- | ------- | ------------ |\\n| subscription\\\\_type         | Chi-Square     | 0.0000  | ‚úÖ Yes        |\\n| payment\\\\_method            | Chi-Square     | 0.0000  | ‚úÖ Yes        |\\n| number\\\\_of\\\\_profiles       | Chi-Square     | 0.0000  | ‚úÖ Yes        |\\n| watch\\\\_hours               | Mann-Whitney U | 0.0000  | ‚úÖ Yes        |\\n| last\\\\_login\\\\_days          | Mann-Whitney U | 0.0000  | ‚úÖ Yes        |\\n| avg\\\\_watch\\\\_time\\\\_per\\\\_day | Mann-Whitney U | 0.0000  | ‚úÖ Yes        |\\n| age                        | Mann-Whitney U | 0.7803  | ‚ùå No         |\\n| gender, region, device     | Chi-Square     | > 0.3   | ‚ùå No         |\\n\\n> ‚úÖ These statistically significant features were included in the final model pipeline.\\n\\n---\\n\\n##  Project Architecture'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '> ‚úÖ These statistically significant features were included in the final model pipeline.\\n\\n---\\n\\n##  Project Architecture\\n\\n```bash\\nnetflix-churn-prediction/\\n‚îú‚îÄ‚îÄ data/                     # Raw and processed data\\n‚îú‚îÄ‚îÄ models/                   # Trained model binaries\\n‚îú‚îÄ‚îÄ reports/                  # Classification reports & plots\\n‚îú‚îÄ‚îÄ static/                   # CSS\\n‚îú‚îÄ‚îÄ templates/                # HTML UI\\n‚îú‚îÄ‚îÄ src/\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py     # Load dataset\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py # Pipeline for scaling & encoding\\n‚îÇ   ‚îî‚îÄ‚îÄ model_training.py     # ML training & evaluation\\n‚îú‚îÄ‚îÄ main.py                   # FastAPI backend\\n‚îú‚îÄ‚îÄ Dockerfile                # Containerization\\n‚îú‚îÄ‚îÄ .dvc/                     # DVC for data version control\\n‚îú‚îÄ‚îÄ .github/workflows/        # CI/CD GitHub Actions\\n‚îî‚îÄ‚îÄ README.md\\n```\\n\\n---\\n\\n## ‚öôÔ∏è End-to-End ML Workflow\\n\\n### 1Ô∏è‚É£ Data Ingestion\\n\\n* Loads `.csv` into DataFrame\\n* Handles errors and logs shape/summary\\n\\n### 2Ô∏è‚É£ Preprocessing'}, page_content='> ‚úÖ These statistically significant features were included in the final model pipeline.\\n\\n---\\n\\n##  Project Architecture\\n\\n```bash\\nnetflix-churn-prediction/\\n‚îú‚îÄ‚îÄ data/                     # Raw and processed data\\n‚îú‚îÄ‚îÄ models/                   # Trained model binaries\\n‚îú‚îÄ‚îÄ reports/                  # Classification reports & plots\\n‚îú‚îÄ‚îÄ static/                   # CSS\\n‚îú‚îÄ‚îÄ templates/                # HTML UI\\n‚îú‚îÄ‚îÄ src/\\n‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py     # Load dataset\\n‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py # Pipeline for scaling & encoding\\n‚îÇ   ‚îî‚îÄ‚îÄ model_training.py     # ML training & evaluation\\n‚îú‚îÄ‚îÄ main.py                   # FastAPI backend\\n‚îú‚îÄ‚îÄ Dockerfile                # Containerization\\n‚îú‚îÄ‚îÄ .dvc/                     # DVC for data version control\\n‚îú‚îÄ‚îÄ .github/workflows/        # CI/CD GitHub Actions\\n‚îî‚îÄ‚îÄ README.md\\n```\\n\\n---\\n\\n## ‚öôÔ∏è End-to-End ML Workflow\\n\\n### 1Ô∏è‚É£ Data Ingestion\\n\\n* Loads `.csv` into DataFrame\\n* Handles errors and logs shape/summary\\n\\n### 2Ô∏è‚É£ Preprocessing'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n## ‚öôÔ∏è End-to-End ML Workflow\\n\\n### 1Ô∏è‚É£ Data Ingestion\\n\\n* Loads `.csv` into DataFrame\\n* Handles errors and logs shape/summary\\n\\n### 2Ô∏è‚É£ Preprocessing\\n\\n* OneHotEncoding (categorical)\\n* StandardScaler (numerical)\\n* Uses `ColumnTransformer` for pipeline modularity\\n\\n### 3Ô∏è‚É£ Model Training\\n\\n* Models: `RandomForest`, `GradientBoosting`, `SVC`\\n* `GridSearchCV` for hyperparameter tuning\\n* Model artifacts saved to `models/`\\n* ROC curves + classification reports saved to `reports/`\\n\\n### 4Ô∏è‚É£ MLflow Tracking ‚úÖ\\n\\n* Tracks experiment metadata, metrics, parameters\\n* Stores models and artifacts\\n* UI accessible at `localhost:5000`\\n\\n---\\n\\n## üß™ Model Performance'}, page_content='---\\n\\n## ‚öôÔ∏è End-to-End ML Workflow\\n\\n### 1Ô∏è‚É£ Data Ingestion\\n\\n* Loads `.csv` into DataFrame\\n* Handles errors and logs shape/summary\\n\\n### 2Ô∏è‚É£ Preprocessing\\n\\n* OneHotEncoding (categorical)\\n* StandardScaler (numerical)\\n* Uses `ColumnTransformer` for pipeline modularity\\n\\n### 3Ô∏è‚É£ Model Training\\n\\n* Models: `RandomForest`, `GradientBoosting`, `SVC`\\n* `GridSearchCV` for hyperparameter tuning\\n* Model artifacts saved to `models/`\\n* ROC curves + classification reports saved to `reports/`\\n\\n### 4Ô∏è‚É£ MLflow Tracking ‚úÖ\\n\\n* Tracks experiment metadata, metrics, parameters\\n* Stores models and artifacts\\n* UI accessible at `localhost:5000`\\n\\n---\\n\\n## üß™ Model Performance'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '### 4Ô∏è‚É£ MLflow Tracking ‚úÖ\\n\\n* Tracks experiment metadata, metrics, parameters\\n* Stores models and artifacts\\n* UI accessible at `localhost:5000`\\n\\n---\\n\\n## üß™ Model Performance\\n\\n| Model             | Accuracy | F1 Score | ROC AUC (Test) | ROC AUC (CV) | Notes                         |\\n| ----------------- | -------- | -------- | -------------- | ------------ | ----------------------------- |\\n| Random Forest     | 0.99     | 0.99     | **0.9995**     | 0.9987       | ‚úÖ Best overall„Äê13‚Ä†source„Äë     |\\n| Gradient Boosting | 0.99     | 0.99     | 0.9989         | 0.9991       | Robust & efficient„Äê12‚Ä†source„Äë |\\n| SVC               | 0.93     | 0.93     | 0.9844         | 0.9822       | Lightweight„Äê14‚Ä†source„Äë        |\\n\\n---\\n\\n## üåê FastAPI Deployment\\n\\n### üîß API Endpoints:\\n\\n* `/`: HTML frontend form for manual input\\n* `/api/predict`: JSON-based API for programmatic inference\\n\\n### üîå Model Used:\\n\\n* Random Forest (best AUC + accuracy)\\n* Accepts form or JSON input\\n* Returns churn prediction + confidence'}, page_content='### 4Ô∏è‚É£ MLflow Tracking ‚úÖ\\n\\n* Tracks experiment metadata, metrics, parameters\\n* Stores models and artifacts\\n* UI accessible at `localhost:5000`\\n\\n---\\n\\n## üß™ Model Performance\\n\\n| Model             | Accuracy | F1 Score | ROC AUC (Test) | ROC AUC (CV) | Notes                         |\\n| ----------------- | -------- | -------- | -------------- | ------------ | ----------------------------- |\\n| Random Forest     | 0.99     | 0.99     | **0.9995**     | 0.9987       | ‚úÖ Best overall„Äê13‚Ä†source„Äë     |\\n| Gradient Boosting | 0.99     | 0.99     | 0.9989         | 0.9991       | Robust & efficient„Äê12‚Ä†source„Äë |\\n| SVC               | 0.93     | 0.93     | 0.9844         | 0.9822       | Lightweight„Äê14‚Ä†source„Äë        |\\n\\n---\\n\\n## üåê FastAPI Deployment\\n\\n### üîß API Endpoints:\\n\\n* `/`: HTML frontend form for manual input\\n* `/api/predict`: JSON-based API for programmatic inference\\n\\n### üîå Model Used:\\n\\n* Random Forest (best AUC + accuracy)\\n* Accepts form or JSON input\\n* Returns churn prediction + confidence'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '### üîå Model Used:\\n\\n* Random Forest (best AUC + accuracy)\\n* Accepts form or JSON input\\n* Returns churn prediction + confidence\\n\\n---\\n\\n## üê≥ Docker Setup\\n\\n```Dockerfile\\nFROM python:3.10-slim\\nWORKDIR /app\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\nCOPY . .\\nEXPOSE 8000\\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\\n```\\n\\nRun locally:\\n\\n```bash\\ndocker build -t netflix-churn .\\ndocker run -p 8000:8000 netflix-churn\\n```\\n\\n---\\n\\n## üîÅ CI/CD Pipeline (GitHub Actions)\\n\\n### ‚úÖ Stages:\\n\\n1. **Test Phase**\\n\\n   * Install dependencies\\n   * Run `pytest` on unit tests\\n   * Pull versioned data using `dvc pull`\\n\\n2. **Build Phase**\\n\\n   * Docker image build with `CACHEBUST` arg\\n   * Push to DockerHub using GitHub Secrets\\n\\n3. **Deploy Phase**\\n\\n   * SSH into EC2 instance\\n   * Stop, remove old container\\n   * Pull and launch updated Docker image\\n\\n### üîê GitHub Repository Secrets'}, page_content='### üîå Model Used:\\n\\n* Random Forest (best AUC + accuracy)\\n* Accepts form or JSON input\\n* Returns churn prediction + confidence\\n\\n---\\n\\n## üê≥ Docker Setup\\n\\n```Dockerfile\\nFROM python:3.10-slim\\nWORKDIR /app\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\nCOPY . .\\nEXPOSE 8000\\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\\n```\\n\\nRun locally:\\n\\n```bash\\ndocker build -t netflix-churn .\\ndocker run -p 8000:8000 netflix-churn\\n```\\n\\n---\\n\\n## üîÅ CI/CD Pipeline (GitHub Actions)\\n\\n### ‚úÖ Stages:\\n\\n1. **Test Phase**\\n\\n   * Install dependencies\\n   * Run `pytest` on unit tests\\n   * Pull versioned data using `dvc pull`\\n\\n2. **Build Phase**\\n\\n   * Docker image build with `CACHEBUST` arg\\n   * Push to DockerHub using GitHub Secrets\\n\\n3. **Deploy Phase**\\n\\n   * SSH into EC2 instance\\n   * Stop, remove old container\\n   * Pull and launch updated Docker image\\n\\n### üîê GitHub Repository Secrets'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '3. **Deploy Phase**\\n\\n   * SSH into EC2 instance\\n   * Stop, remove old container\\n   * Pull and launch updated Docker image\\n\\n### üîê GitHub Repository Secrets\\n\\n| Name                    | Purpose                            |\\n| ----------------------- | ---------------------------------- |\\n| `AWS_ACCESS_KEY_ID`     | AWS auth for DVC S3                |\\n| `AWS_SECRET_ACCESS_KEY` | AWS auth for DVC S3                |\\n| `DOCKER_USERNAME`       | DockerHub username for push        |\\n| `DOCKER_PASSWORD`       | DockerHub password/token           |\\n| `EC2_HOST`              | Public IP/DNS of EC2 instance      |\\n| `EC2_USER`              | SSH user for EC2 login             |\\n| `EC2_SSH_KEY`           | Private SSH key for GitHub Actions |\\n\\n---\\n\\n## üß¨ Data Versioning with DVC\\n\\n* Tracks raw and preprocessed data versions\\n* Uses `.dvc/config` to connect to **AWS S3** remote\\n* Run `dvc push` and `dvc pull` to sync across environments\\n* Ensures reproducibility in CI and local experiments\\n\\n---'}, page_content='3. **Deploy Phase**\\n\\n   * SSH into EC2 instance\\n   * Stop, remove old container\\n   * Pull and launch updated Docker image\\n\\n### üîê GitHub Repository Secrets\\n\\n| Name                    | Purpose                            |\\n| ----------------------- | ---------------------------------- |\\n| `AWS_ACCESS_KEY_ID`     | AWS auth for DVC S3                |\\n| `AWS_SECRET_ACCESS_KEY` | AWS auth for DVC S3                |\\n| `DOCKER_USERNAME`       | DockerHub username for push        |\\n| `DOCKER_PASSWORD`       | DockerHub password/token           |\\n| `EC2_HOST`              | Public IP/DNS of EC2 instance      |\\n| `EC2_USER`              | SSH user for EC2 login             |\\n| `EC2_SSH_KEY`           | Private SSH key for GitHub Actions |\\n\\n---\\n\\n## üß¨ Data Versioning with DVC\\n\\n* Tracks raw and preprocessed data versions\\n* Uses `.dvc/config` to connect to **AWS S3** remote\\n* Run `dvc push` and `dvc pull` to sync across environments\\n* Ensures reproducibility in CI and local experiments\\n\\n---'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\n## üìå Business Value & Insights\\n\\n* üß† **High-risk churn users** are linked to:\\n\\n  * Low engagement (low watch hours)\\n  * Infrequent logins\\n  * Basic plans & non-card payments\\n\\n* üìà **Operational Benefits**:\\n\\n  * Preemptive retention campaigns\\n  * Personalized offers to vulnerable users\\n  * Reduce marketing costs via targeted outreach\\n\\n---\\n\\n## ‚úÖ Run Locally (No Docker)\\n\\n```bash\\ngit clone <repo_url>\\ncd netflix-churn-prediction\\npython src/model_training.py        # Train all models\\nuvicorn main:app --reload           # Launch API server\\n```\\n\\n---\\n## Summary'}, page_content='---\\n\\n## üìå Business Value & Insights\\n\\n* üß† **High-risk churn users** are linked to:\\n\\n  * Low engagement (low watch hours)\\n  * Infrequent logins\\n  * Basic plans & non-card payments\\n\\n* üìà **Operational Benefits**:\\n\\n  * Preemptive retention campaigns\\n  * Personalized offers to vulnerable users\\n  * Reduce marketing costs via targeted outreach\\n\\n---\\n\\n## ‚úÖ Run Locally (No Docker)\\n\\n```bash\\ngit clone <repo_url>\\ncd netflix-churn-prediction\\npython src/model_training.py        # Train all models\\nuvicorn main:app --reload           # Launch API server\\n```\\n\\n---\\n## Summary'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '| **Component**            | **Implemented** | **Tool/Service Used**                     |\\n| ------------------------ | --------------- | ----------------------------------------- |\\n| **Data Versioning**      | ‚úÖ               | `DVC` with `AWS S3` remote                |\\n| **Data Ingestion**       | ‚úÖ               | `pandas`, custom Python class             |\\n| **Data Preprocessing**   | ‚úÖ               | `scikit-learn` Pipelines                  |\\n| **Model Training**       | ‚úÖ               | `scikit-learn`, `GridSearchCV`            |\\n| **Experiment Tracking**  | ‚úÖ               | `MLflow` (local server: `127.0.0.1:5000`) |\\n| **Model Evaluation**     | ‚úÖ               | `classification_report`, ROC AUC          |\\n| **Model Packaging**      | ‚úÖ               | `joblib` for serialization                |\\n| **API Deployment**       | ‚úÖ               | `FastAPI` on `AWS EC2`                    |'}, page_content='| **Component**            | **Implemented** | **Tool/Service Used**                     |\\n| ------------------------ | --------------- | ----------------------------------------- |\\n| **Data Versioning**      | ‚úÖ               | `DVC` with `AWS S3` remote                |\\n| **Data Ingestion**       | ‚úÖ               | `pandas`, custom Python class             |\\n| **Data Preprocessing**   | ‚úÖ               | `scikit-learn` Pipelines                  |\\n| **Model Training**       | ‚úÖ               | `scikit-learn`, `GridSearchCV`            |\\n| **Experiment Tracking**  | ‚úÖ               | `MLflow` (local server: `127.0.0.1:5000`) |\\n| **Model Evaluation**     | ‚úÖ               | `classification_report`, ROC AUC          |\\n| **Model Packaging**      | ‚úÖ               | `joblib` for serialization                |\\n| **API Deployment**       | ‚úÖ               | `FastAPI` on `AWS EC2`                    |'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '| **Model Packaging**      | ‚úÖ               | `joblib` for serialization                |\\n| **API Deployment**       | ‚úÖ               | `FastAPI` on `AWS EC2`                    |\\n| **Web UI**               | ‚úÖ               | HTML + Bootstrap via Jinja2               |\\n| **Containerization**     | ‚úÖ               | `Docker` (with `Dockerfile`)              |\\n| **CI/CD Pipeline**       | ‚úÖ               | `GitHub Actions`                          |\\n| **Cloud Hosting**        | ‚úÖ               | `AWS EC2`, SSH-based deployment           |\\n| **Secrets Management**   | ‚úÖ               | `GitHub Secrets`                          |\\n| **Testing**              | ‚úÖ               | `pytest`, CI-tested                       |'}, page_content='| **Model Packaging**      | ‚úÖ               | `joblib` for serialization                |\\n| **API Deployment**       | ‚úÖ               | `FastAPI` on `AWS EC2`                    |\\n| **Web UI**               | ‚úÖ               | HTML + Bootstrap via Jinja2               |\\n| **Containerization**     | ‚úÖ               | `Docker` (with `Dockerfile`)              |\\n| **CI/CD Pipeline**       | ‚úÖ               | `GitHub Actions`                          |\\n| **Cloud Hosting**        | ‚úÖ               | `AWS EC2`, SSH-based deployment           |\\n| **Secrets Management**   | ‚úÖ               | `GitHub Secrets`                          |\\n| **Testing**              | ‚úÖ               | `pytest`, CI-tested                       |'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n## üôå Author\\n\\n* üë®\\u200düíª Katta Sai Pranav Reddy\\n\\n---\\n## üîó Links\\n\\nüîç GitHub Repo: Netflix-Customer-Churn-Prediction-Using-Machine-Learning\\n\\nüê≥ DockerHub: pranavreddy123/netflix-churn-prediction\\n\\n## üìé Tech Stack\\n\\n* **Python 3.10**\\n* **Scikit-learn**, **MLflow**, **DVC**, **FastAPI**, **Docker**\\n* **GitHub Actions**, **AWS EC2**, **S3 Remote Storage**\\n---\\n\\n### **Skills**\\n\\n* **Tools:** MLflow, DVC, Docker, Git, GitHub Actions, AWS (EC2, S3, ECR), FAISS, Pinecone, Hugging Face, LangChain, LangSmith, FastAPI\\n* **Programming & Technical Skills:** Python, SQL, HTML, CSS, Scikit-learn, TensorFlow, Keras, Statistics\\n* **Data Science & Machine Learning:** Data Preprocessing, EDA, Feature Engineering, Model Training & Evaluation, Hyperparameter Tuning, Clustering, MLOps, Semantic Search, Retrieval-Augmented Generation (RAG), CNN, RNN, GPT, Transformers, Fine-Tuning, Prompt Engineering\\n* **Data Visualization & Analysis:** Pandas, NumPy, Matplotlib, Seaborn\\n\\n---\\n\\nhobbies section \\n\\n---'}, page_content='---\\n## üôå Author\\n\\n* üë®\\u200düíª Katta Sai Pranav Reddy\\n\\n---\\n## üîó Links\\n\\nüîç GitHub Repo: Netflix-Customer-Churn-Prediction-Using-Machine-Learning\\n\\nüê≥ DockerHub: pranavreddy123/netflix-churn-prediction\\n\\n## üìé Tech Stack\\n\\n* **Python 3.10**\\n* **Scikit-learn**, **MLflow**, **DVC**, **FastAPI**, **Docker**\\n* **GitHub Actions**, **AWS EC2**, **S3 Remote Storage**\\n---\\n\\n### **Skills**\\n\\n* **Tools:** MLflow, DVC, Docker, Git, GitHub Actions, AWS (EC2, S3, ECR), FAISS, Pinecone, Hugging Face, LangChain, LangSmith, FastAPI\\n* **Programming & Technical Skills:** Python, SQL, HTML, CSS, Scikit-learn, TensorFlow, Keras, Statistics\\n* **Data Science & Machine Learning:** Data Preprocessing, EDA, Feature Engineering, Model Training & Evaluation, Hyperparameter Tuning, Clustering, MLOps, Semantic Search, Retrieval-Augmented Generation (RAG), CNN, RNN, GPT, Transformers, Fine-Tuning, Prompt Engineering\\n* **Data Visualization & Analysis:** Pandas, NumPy, Matplotlib, Seaborn\\n\\n---\\n\\nhobbies section \\n\\n---'),\n",
       " Document(metadata={'source': '../data/info.txt', 'text': '---\\n\\nhobbies section \\n\\n---\\n\\n### **Hobbies & Interests**\\nHobbies & Interests\\n\\n* Playing Cricket\\n* Watching Football\\n* Reading Books\\n* Exploring Latest Advancements in Artificial Intelligence\\n* Browsing the Internet for Tech & Knowledge Updates\\n\\n---\\n\\n### Contact Information\\n\\nContact Information\\n\\nüìû Phone: +91 93475 41040\\nüìß Email: [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\\nüíª GitHub: [github.com/ka1817](https://github.com/ka1817)\\nüîó LinkedIn: [linkedin.com/in/pranav-reddy-katta](https://www.linkedin.com/in/pranav-reddy-katta/)\\n---\\n\\n---\\nCertifications By Pranav Reddy\\nCertifications:  \\n(All from Udemy)  \\n- Python for Data Science and Machine Learning  \\n- The Complete SQL Bootcamp  \\n- Generative AI with LangChain and HuggingFace  \\n- End-To-End MLOps Bootcamp  \\n\\n---'}, page_content='---\\n\\nhobbies section \\n\\n---\\n\\n### **Hobbies & Interests**\\nHobbies & Interests\\n\\n* Playing Cricket\\n* Watching Football\\n* Reading Books\\n* Exploring Latest Advancements in Artificial Intelligence\\n* Browsing the Internet for Tech & Knowledge Updates\\n\\n---\\n\\n### Contact Information\\n\\nContact Information\\n\\nüìû Phone: +91 93475 41040\\nüìß Email: [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\\nüíª GitHub: [github.com/ka1817](https://github.com/ka1817)\\nüîó LinkedIn: [linkedin.com/in/pranav-reddy-katta](https://www.linkedin.com/in/pranav-reddy-katta/)\\n---\\n\\n---\\nCertifications By Pranav Reddy\\nCertifications:  \\n(All from Udemy)  \\n- Python for Data Science and Machine Learning  \\n- The Complete SQL Bootcamp  \\n- Generative AI with LangChain and HuggingFace  \\n- End-To-End MLOps Bootcamp  \\n\\n---')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ada7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Answer ===\n",
      "Here are the contact details:\n",
      "\n",
      "üìû Phone: +91 93475 41040\n",
      "üìß Email: [kattapranavreddy@gmail.com](mailto:kattapranavreddy@gmail.com)\n",
      "üíª GitHub: [github.com/ka1817](https://github.com/ka1817)\n",
      "üîó LinkedIn: [linkedin.com/in/pranav-reddy-katta](https://www.linkedin.com/in/pranav-reddy-katta/)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.get_relevant_documents(query)  # ‚úÖ FIX\n",
    "        if not initial_docs:\n",
    "            return []\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=5)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=custom_retriever,\n",
    "    chain_type=\"stuff\"  \n",
    ")\n",
    "\n",
    "query = \"Give me the contact details\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "print(\"\\n=== Answer ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "372f94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Answer ===\n",
      "Pranav Reddy has completed the following certifications, all of which are from Udemy:\n",
      "\n",
      "1. **Python for Data Science and Machine Learning**\n",
      "2. **The Complete SQL Bootcamp**\n",
      "3. **Generative AI with LangChain and HuggingFace**\n",
      "4. **End-To-End MLOps Bootcamp**\n",
      "\n",
      "These certifications demonstrate his interest and expertise in areas such as data science, machine learning, SQL, and artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.get_relevant_documents(query)\n",
    "        if not initial_docs:\n",
    "            return []\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=5)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an expert assistant. Use the following context to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a clear, structured, and detailed manner. If the context does not contain the answer,\n",
    "say \"I don't know based on the given context.\"\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=custom_retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt}\n",
    ")\n",
    "\n",
    "query = \"what certifications pranav reddy have done?\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "print(\"\\n=== Answer ===\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a58e961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Answer ===\n",
      "Based on the given context, Pranav Reddy has completed the following certifications, all of which are from Udemy:\n",
      "\n",
      "1. **Python for Data Science and Machine Learning**\n",
      "2. **The Complete SQL Bootcamp**\n",
      "3. **Generative AI with LangChain and HuggingFace**\n",
      "4. **End-To-End MLOps Bootcamp**\n",
      "\n",
      "These certifications are mentioned in multiple documents within the context, indicating that they are a part of Pranav Reddy's educational background and skills.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "# ‚úÖ Reranker\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    docs_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [(query, doc_text) for doc_text in docs_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "class RerankRetriever(BaseRetriever, BaseModel):\n",
    "    base_retriever: BaseRetriever\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.base_retriever.get_relevant_documents(query)\n",
    "        if not initial_docs:\n",
    "            return []\n",
    "        reranked_docs = rerank_documents(query, initial_docs)\n",
    "        return reranked_docs[:self.top_k]\n",
    "\n",
    "# ‚úÖ Build reranking retriever\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "custom_retriever = RerankRetriever(base_retriever=base_retriever, top_k=5)\n",
    "\n",
    "# ‚úÖ Custom Prompt\n",
    "prompt_template = \"\"\"\n",
    "You are an expert assistant. Use the following context to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a clear, structured, and detailed manner. If the context does not contain the answer,\n",
    "say \"I don't know based on the given context.\"\n",
    "\"\"\"\n",
    "custom_prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ‚úÖ LCEL RAG pipeline\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": custom_retriever,   # returns reranked docs\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | custom_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ‚úÖ Ask question\n",
    "query = \"what certifications pranav reddy have done?\"\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(\"\\n=== Answer ===\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e53657b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_query(\"who is kohli\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"‚ùå Missing PINECONE_API_KEY in environment variables\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
